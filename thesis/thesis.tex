%!TEX root = thesis.tex

\documentclass[12pt]{article}
\usepackage[spanish]{babel}

% Margenes
\usepackage{geometry}
\geometry{
  letterpaper,
  top=4cm,
  bottom=3cm,
  left=4cm,
  right=3cm
}

% Fuente
\usepackage{fontspec}
\setmainfont{Times New Roman}
\usepackage{csquotes}

\renewcommand{\baselinestretch}{1.5}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{parskip}
\usepackage[spanish]{cleveref}

\usepackage[caption = true]{subfig}

\usepackage[backend=biber,style=apa,language=spanish]{biblatex}
\bibliography{biblio}

\begin{document}

\title{Análisis Descriptivo y Predictivo Basado en Datos del Tráfico Vehicular en Antofagasta: Un Enfoque a partir de Reportes de Conductores}
\author{Richard Peña Bonifaz}
\date{\today}
\maketitle

% \section*{Dedicatoria}
% \section*{Reconocimientos}
% \tableofcontents
% \newpage

\section*{Resumen}

El presente proyecto tiene como propósito desarrollar una herramienta efectiva para analizar y predecir, con alta confiabilidad, el comportamiento del tráfico vehicular en la ciudad de Antofagasta mediante el uso de datos provenientes de la plataforma Waze Cities. Waze, a través de su comunidad de usuarios, ofrece información en tiempo real que permite obtener una visión detallada de los eventos de tráfico que ocurren en la ciudad. Esta investigación genera información relevante para la gestión del tráfico, facilitando la toma de decisiones por parte de las autoridades locales con el fin de mejorar la seguridad vial y optimizar la eficiencia del flujo vehicular. A través del análisis y explotación de estos datos, se identifican patrones y tendencias que, integrados en la planificación urbana, contribuirán a optimizar rutas críticas, reducir la congestión y minimizar la probabilidad de accidentes.

En este estudio se emplearán técnicas de análisis de datos y métodos geoespaciales mediante el uso de la librería GeoPandas en Python, además de la aplicación de series temporales para proporcionar visualizaciones claras y comprensibles dirigidas al usuario final. Adicionalmente, se implementarán técnicas de aprendizaje automático para realizar predicciones de tráfico, anticipándose a eventos y contribuyendo al desarrollo urbano eficiente y seguro \parencite{barcelo2005}.

\section{Introducción}
\subsection{Descripción del problema}

Antofagasta, una ciudad con más de 106,000 vehículos en circulación \parencite{comision2023}, enfrenta desafíos significativos en la gestión de su tráfico vehicular. Durante el año 2023, se registraron 1,715 accidentes, los cuales resultaron en 31 fallecidos y 102 heridos graves \parencite{comision2023}. La infraestructura vial limitada, sumada a la alta concentración de vehículos en un número reducido de arterias principales, agravó la congestión y elevó el riesgo de accidentes. A pesar de la existencia de estos problemas, no se cuenta con sistemas de monitoreo en tiempo real que permitan gestionar el tráfico de manera proactiva. Por ello, se aprovecharán fuentes de datos alternativas, como Waze, para recolectar información valiosa que facilite la toma de decisiones en materia de tráfico \parencite{chen2015}.

\subsection{Objetivo general}

Realizar un análisis exhaustivo del comportamiento del tráfico en la ciudad de Antofagasta basado en los eventos reportados por los conductores en la plataforma Waze. El objetivo final es generar información relevante que contribuya a la gestión eficiente del tráfico, mejorando la seguridad vial y optimizando el flujo vehicular.

\subsection{Objetivos específicos}

\begin{itemize}
    \item Obtener datos suficientes y representativos sobre los eventos de tráfico en Antofagasta mediante la API de Waze Cities.
    \item Realizar un análisis descriptivo de los datos recolectados para identificar patrones y tendencias relevantes en el comportamiento del tráfico.
    \item Identificar los factores clave que influyen en la seguridad vial y la eficiencia del tráfico en la ciudad.
    \item Presentar información visualmente comprensible y útil para las autoridades de gestión vial, facilitando la implementación de políticas y acciones basadas en datos \parencite{auld2009}.
    \item Utilizar los datos disponibles para desarrollar modelos predictivos de tráfico que permitan anticipar eventos y tomar decisiones proactivas en la gestión vial.
\end{itemize}

\section{Marco teórico}

El tráfico vehicular en entornos urbanos presenta un comportamiento complejo e impredecible, lo que dificulta su gestión eficiente. No obstante, el avance de las tecnologías móviles y la popularidad de aplicaciones como Waze permiten disponer de datos en tiempo real generados por los propios usuarios. Este proyecto se apoya en técnicas de análisis de datos y aprendizaje de máquinas (Machine Learning) para convertir esta información en herramientas útiles para la gestión vial. La utilización de datos geoespaciales, junto con la automatización de los procesos de recolección, análisis y visualización, constituye una solución costo-efectiva para mejorar la planificación del tráfico \parencite{barcelo2005}.

\subsection{Metodología}

Una de las principales limitaciones en el análisis de fenómenos es la ausencia de datos suficientes y debidamente estructurados. Por ello, se diseñó una estrategia de recolección de datos que garantizara conclusiones con un nivel adecuado de certeza. La API de Waze Cities, que proporciona información en tiempo real sobre eventos activos, fue la fuente principal de datos. Para lograr un volumen de datos representativo, se implementó un servidor encargado de recopilar y almacenar esta información de manera continua.

Se realizó un análisis geoespacial con el objetivo de identificar puntos críticos, como vías principales, calles secundarias y zonas de alto tráfico. Este análisis se llevó a cabo utilizando GeoPandas, una herramienta de Python especializada en operaciones geoespaciales. Además, se analizaron series temporales para detectar patrones y tendencias del tráfico, identificando estacionalidades en el comportamiento. Los resultados se presentaron mediante técnicas de visualización que permitieron interpretar las tendencias y puntos de interés de manera efectiva.

El pipeline de datos incluyó una base de datos relacional para almacenar los datos, un flujo ETL (Extract, Transform, Load) para procesarlos y un servidor web para visualizarlos. Se utilizó PostgreSQL como base de datos, APScheduler para la programación de tareas y Dash como herramienta de visualización. Este enfoque permitió automatizar el flujo de datos y garantizar la actualización constante de la información.

Adicionalmente, se entrenó un modelo de clasificación para determinar la probabilidad de ocurrencia de accidentes en diferentes puntos de la ciudad. El modelo seleccionado fue XGBoost, el cual fue seleccionado utilizando técnicas de validación cruzada y optimización de hiperparámetros. Los resultados obtenidos permitieron identificar las variables más influyentes en la ocurrencia de accidentes, así como la probabilidad de ocurrencia en diferentes condiciones de tráfico. Para la selección de variables se utilizó GridSearchCV, una técnica de búsqueda de hiperparámetros que permite encontrar la mejor combinación de variables para el modelo, se compararon modelos de regresión logística, árboles de decisión y XGBoost, siendo este último el que presentó el mejor desempeño en términos de precisión y sensibilidad.

Para la gestión del modelo, en cuanto a su implementación, mantenimiento y actualización, se programó con APSCheduler, una librería de tareas asíncronas en Python, que permite gestionar el ciclo de vida de los modelos de aprendizaje automático, desde su entrenamiento hasta su despliegue en producción y versionado.

En la parte del servidor, se desarrolló en Rust, un lenguaje de programación enfocado en la eficiencia en el uso de memoria y seguridad en el uso de la misma. Se utilizó la herramienta Memcached para generar un caché de los datos y de esta forma poder servirlos de manera más eficiente y rápida, debido al volumen de dato, se priorizó la unicidad de los datos, evitando generar copias, para no aumentar el uso de memoria.

Para el procesado de los datos en el cliente (Dash), se generan multiples workers que permiten servir la aplicación de forma eficiente, y se utiliza solo una instancia en memoria de los datos, compartidos por todos los datos, y para mantener la integridad de los datos, se implementó un Mutex para controlar el acceso de lectura a los mismos desde las tareas asíncronas.

La perspectiva general del flujo de datos se muestra en la \autoref{fig:fuente} y el flujo desde la API de Waze hasta el dashboard se puede observar en la \autoref{fig:wf_dash}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/fuente_datos.png}
    \caption{Pipeline general de datos}
    \label{fig:fuente}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/wf_dash.png}
    \caption{Flujo de información en dashboard}
    \label{fig:wf_dash}
\end{figure}

\section{Desarrollo}

Para obtener los datos necesarios para el análisis, fue necesario generar un mecanismo para recolectar y almacenar la información de Waze Cities. Se implementó un servidor que se encarga de realizar peticiones a la API de Waze, recolectar los eventos de tráfico y almacenarlos en una base de datos PostgreSQL. Este servidor se ejecutaba de manera continua, actualizando la información cada 2 minutos. La estructura de la base de datos se muestra en la \autoref{fig:db_diagram}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/db_diagram.png}
    \caption{Diagrama de la base de datos}
    \label{fig:db_diagram}
\end{figure}

Existen dos estructuras principales, las alertas (alerts) y las congestiones (jams), el primero registra todos los eventos reportados por usuarios, tales como peligros, accidentes, congestión o rutas cerradas. En el caso de los eventos de congestión, son datos generados automáticamente por Waze, los cuales usando la geolocalización, estiman congestiones en diferentes puntos de la ciudad. Para este proyecto, se utilizó la información de las alertas. Sin embargo, se almacenaron los datos de congestión, para un posterior análisis, que está fuera del alcance de este estudio. Esta estructura se definió con base en los datos relevantes y completos que proporciona la API, los tipos de datos disponibles pueden ser consultados en la documentación de Waze \parencite{waze2024}.

Desde octubre de 2024 hasta Abril de 2025, se han acumulado 40429 alertas, las cuales se analizarán a lo largo de esde estudio.


\subsection{Eventos en el espacio y tiempo}

En la \autoref{fig:dist_events} se observa la distribución espacial de los distintos tipos de eventos en la ciudad. Se aprecia una mayor concentración en las principales avenidas, especialmente en el caso de los accidentes, los cuales tienden a ocurrir con mayor frecuencia en los cruces de estas arterias.

\begin{figure}[H]
    \centering
    \subfloat[Accidentes]{
        \includegraphics[width=0.33\textwidth]{images/dist_accidents.png}
    }
    \subfloat[Congestión]{
        \includegraphics[width=0.33\textwidth]{images/dist_jams.png}
    }
    \subfloat[Peligros]{
        \includegraphics[width=0.33\textwidth]{images/dist_hazards.png}
    }
    \caption{Distribución espacial de eventos en Antofagasta}
    \label{fig:dist_events}
\end{figure}

Para el análisis temporal, los eventos se agruparon por hora del día, calculando el promedio de eventos ocurridos por jornada. En la \autoref{fig:time_events} se evidencia una concentración de accidentes y congestiones durante las horas punta. En contraste, los eventos de tipo peligro presentan una distribución más uniforme a lo largo del día.

\begin{figure}[H]
    \centering
    \subfloat[Accidentes]{
        \includegraphics[width=0.6\textwidth]{images/ACCIDENT_per_hour.png}
    }
    \newline
    \subfloat[Congestión]{
        \includegraphics[width=0.6\textwidth]{images/JAM_per_hour.png}
    }
    \newline
    \subfloat[Peligros]{
        \includegraphics[width=0.6\textwidth]{images/HAZARD_per_hour.png}
    }
    \newline
    \caption{Eventos según hora del día en Antofagasta}
    \label{fig:time_events}
\end{figure}

% Prueba hipótesis relación accidents-jams


\begin{figure}[H]
    \centering
    \subfloat[Día de semana]{
        \includegraphics[width=0.9\textwidth]{images/corr_lin_ds.png}
    }
    \newline
    \subfloat[Fin de semana]{
        \includegraphics[width=0.9\textwidth]{images/corr_lin_fs.png}
    }
    \newline
    \caption{Correlación lineal entre accidentes y congestión}
    \label{fig:corr_lineal}
\end{figure}


\begin{figure}[H]
    \centering
    \subfloat[Día de semana]{
        \includegraphics[width=0.9\textwidth]{images/corr_exp_ds.png}
    }
    \newline
    \subfloat[Fin de semana]{
        \includegraphics[width=0.9\textwidth]{images/corr_exp_fs.png}
    }
    \newline
    \caption{Correlación exponencial inversa entre accidentes y congestión}
    \label{fig:corr_exp}
\end{figure}

% Cálculo de la frecuencia por zona
% Filtro por zona
%    Algóritmo cKDTree

\subsection{Frecuencia por zona}

Para representar conjuntos estáticos de puntos espaciales, se utiliza la estructura cKd-tree, una variante compacta del Kd-tree que codifica distancias relativas mediante spiral codes y las almacena usando Directly Addressable Codes (DACs), optimizando el uso de memoria sin sacrificar eficiencia en consultas espaciales \parencite{gutierrez2023ckdtree}. A cada zona la llamaremos cuadrante o segmento.

En la \autoref{fig:quad_acc} puede verse como se distribuyen los segmentos númericamente, partiendo desde el segmento 1 en la esquina inferior izquierda. La opacidad del color rojo va en relación con la cantidad de accidentes por segmento.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.38\textwidth]{images/quad_acc.png}
    \caption{Identificación de cuadrantes en mapa de Antofagasta}
    \label{fig:quad_acc}
\end{figure}

Tomamos la media de accidentes diarios, los agrupamos por segmento, definimos que las zonas mayores a 0.8 accidentes por día son considerados críticos, construimos un mapa según la criticidad (\autoref{fig:quad_mean_day}), si tomamos la suma de los accidentes, tenemos la \autoref{fig:quad_acc_total} que muestra la cantidad de accidentes reportados en el segmento en el periodo de estudio, que a su vez se clasifican como críticos si poseen más de 140 accidentes.


\begin{figure}[H]
    \centering
    \subfloat[Media de accidentes por día]{
        \includegraphics[width=0.5\textwidth]{images/quad_mean_day_color.png}
        \label{fig:quad_mean_day}
    }
    \subfloat[Total accidentes]{
        \includegraphics[width=0.5\textwidth]{images/quad_acc_color.png}
        \label{fig:quad_acc_total}
    }
    \caption{Accidentes por segmento}
\end{figure}


% Ruta más segura

% Machine learning (XGBoost)

\subsection{Selección del modelo}

Se evaluaron distintos algoritmos de clasificación para abordar el problema planteado. Inicialmente se probaron modelos clásicos como \textit{Logistic Regression}, \textit{K-Nearest Neighbors}, \textit{Decision Tree}, entre otros. Sin embargo, estos no mostraron mejoras relevantes en las métricas evaluadas, por lo que se descartaron para la fase de ajuste de hiperparámetros.

Se procedió entonces a aplicar una búsqueda exhaustiva (\textit{GridSearchCV}) sobre dos modelos con buen desempeño preliminar: \textbf{Random Forest} y \textbf{XGBoost}.

\paragraph{Random Forest.}
Se evaluaron los siguientes hiperparámetros:

\begin{itemize}
    \item \texttt{max\_depth}: [2, 5, 10]
    \item \texttt{n\_estimators}: [20, 80, 100, 200]
    \item \texttt{min\_samples\_leaf}: [2, 4, 8]
    \item \texttt{max\_leaf\_nodes}: [2, 8]
    \item \texttt{max\_features}: [2, 8]
    \item \texttt{class\_weight}: [None, 'balanced']
\end{itemize}

Los mejores hiperparámetros obtenidos fueron:

\begin{itemize}
    \item \texttt{class\_weight}: None
    \item \texttt{max\_depth}: 10
    \item \texttt{max\_features}: 2
    \item \texttt{max\_leaf\_nodes}: 8
    \item \texttt{min\_samples\_leaf}: 2
    \item \texttt{n\_estimators}: 100
\end{itemize}

Con estos valores, el modelo alcanzó una puntuación máxima de \textbf{0.7957} en validación cruzada.


\paragraph{XGBoost.}
Se aplicaron múltiples rondas de \textit{grid search} con distintos rangos de hiperparámetros, refinando progresivamente los valores. Los mejores resultados se obtuvieron con:

\begin{itemize}
    \item \texttt{colsample\_bytree}: 0.7
    \item \texttt{gamma}: 1
    \item \texttt{learning\_rate}: 0.1
    \item \texttt{max\_depth}: 30
    \item \texttt{n\_estimators}: 80
\end{itemize}

Este modelo logró una métrica de validación cruzada de \textbf{0.8665}, superando significativamente al resto de las alternativas evaluadas. Por este motivo, \textbf{XGBoost fue seleccionado como modelo final}.

En el \autoref{tab:model-comparison} se puede observar la comparación de ambos modelos.

\begin{table}[H]
\centering
\begin{tabular}{|l|p{9cm}|c|}
\hline
\textbf{Modelo} & \textbf{Mejores Hiperparámetros} & \textbf{Mejor Score} \\
\hline
\textbf{Random Forest} &
\begin{tabular}[c]{@{}l@{}}
\texttt{class\_weight}: None\\
\texttt{max\_depth}: 10\\
\texttt{max\_features}: 2\\
\texttt{max\_leaf\_nodes}: 8\\
\texttt{min\_samples\_leaf}: 2\\
\texttt{n\_estimators}: 100
\end{tabular}
& 0.7957 \\
\hline
\textbf{XGBoost} &
\begin{tabular}[c]{@{}l@{}}
\texttt{colsample\_bytree}: 0.7\\
\texttt{gamma}: 1\\
\texttt{learning\_rate}: 0.1\\
\texttt{max\_depth}: 30\\
\texttt{n\_estimators}: 80
\end{tabular}
& \textbf{0.8665} \\
\hline
\end{tabular}
\caption{Comparativa de modelos ajustados con GridSearchCV}
\label{tab:model-comparison}
\end{table}


\subsection{Generación de eventos negativos simulados}

Dado que el conjunto original contenía únicamente eventos positivos (ocurridos), fue necesario generar instancias negativas (no ocurridos) para entrenar un modelo de clasificación binaria. Para ello, se implementó una estrategia inspirada en el enfoque de generación de eventos negativos artificiales propuesto por Goedertier et al. (2009), que permite representar problemas secuenciales como tareas de clasificación supervisada.

En nuestro caso, la generación se basa en una grilla temporal con intervalos de 5 minutos, sobre la cual se combinan todas las posibles ubicaciones (\texttt{group}) y tipos de evento. Luego, se identifican aquellas combinaciones que no se encuentran en los datos originales (es decir, donde no ocurrió un evento) y se etiquetan como eventos negativos (\texttt{happen} = 0). Estas instancias se muestrean de forma aleatoria para igualar la cantidad de eventos positivos, logrando así un conjunto balanceado con un 50\% de eventos positivos y 50\% de negativos.

Este enfoque permite mantener la estructura temporal y geoespacial de los datos reales, evitando introducir ruido aleatorio, y facilitando la evaluación robusta de métricas como \texttt{recall} y \texttt{precision} \parencite{goedertier2009robust}.


\subsection{Resultados del modelo seleccionado}

La versión final del modelo (\texttt{v6}) se entrenó utilizando los hiperparámetros obtenidos durante la etapa de validación cruzada. El modelo fue registrado y versionado mediante \textit{MLflow}, permitiendo un seguimiento detallado de sus parámetros y métricas.

\noindent Los parámetros más relevantes del modelo fueron:

\begin{itemize}
    \item \texttt{max\_depth} = 20
    \item \texttt{n\_estimators} = 80
    \item \texttt{learning\_rate} = 0.1
    \item \texttt{gamma} = 0.8
    \item \texttt{colsample\_bytree} = 0.7
\end{itemize}

El modelo utiliza codificación \textit{one-hot} y un esquema de clasificación binaria (\texttt{binary:logistic}), con una semilla aleatoria (\texttt{random\_state = 42}) para garantizar la reproducibilidad.


\noindent En cuanto al rendimiento, se obtuvieron los siguientes resultados para una muestra de 64.708 eventos, con un 50\% de eventos ocurridos y un 50\% de no ocurridos. Esta proporción se logró mediante la simulación de eventos negativos sobre una grilla temporal de 5 minutos, generando combinaciones posibles de tiempo, localización y tipo de evento que no están presentes en los datos originales. Esto permite evaluar el modelo en condiciones balanceadas, facilitando la interpretación de métricas como \texttt{precision}, \texttt{recall} y \texttt{F1-score}.

\begin{itemize}
    \item \textbf{Accuracy}: 0.8795
    \item \textbf{Recall}: 0.9076
    \item \textbf{Precision}: 0.8583
    \item \textbf{F1-score}: 0.8823
    \item \textbf{Promedio validación cruzada (10 folds)}: 0.7819
\end{itemize}

\noindent Respecto a la matriz de confusión, se registraron los siguientes valores:

\begin{itemize}
    \item Verdaderos positivos: 5,539
    \item Verdaderos negativos: 5,843
    \item Falsos positivos: 965
    \item Falsos negativos: 595
\end{itemize}

Estas métricas evidencian un modelo con excelente capacidad para identificar correctamente los casos positivos, sin sacrificar precisión ni balance. La validación cruzada muestra estabilidad a lo largo de los distintos folds, lo que refuerza la robustez del modelo final.

\subsection{Curva de aprendizaje del modelo}

La \autoref{fig:learning_curve} muestra la curva de aprendizaje del modelo XGBoost entrenado. Se observa un rendimiento alto y consistente tanto en el conjunto de entrenamiento como en el de validación (aproximadamente 0.88–0.90), con una brecha mínima entre ambas curvas. Esto indica que el modelo generaliza adecuadamente, sin presentar sobreajuste ni subajuste. Además, el comportamiento plano de ambas curvas sugiere que incrementar el tamaño del conjunto de entrenamiento no tendría un impacto significativo en el rendimiento, lo cual evidencia una saturación en la capacidad de aprendizaje del modelo con las características actuales.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/learning_curve.png}
    \caption{Identificación de cuadrantes en mapa de Antofagasta}
    \label{fig:learning_curve}
\end{figure}


% Aplicación implementacióñ (dashboard online y ML)

% \section{Resultados}

% Observaciones y resultados del estudio

% \section{Discusión}

% Preguntas y posibles soluciones

% \section{Conclusiones}

% Conclusión según los resultados

% \section{Recomendaciones}

% Propuesta de solución o nuevo estudio

\printbibliography

\end{document}

