%!TEX root = thesis.tex

\documentclass[12pt]{article}
<<<<<<< HEAD
\usepackage[spanish,es-noshorthands,es-tabla]{babel}
=======
\usepackage[spanish,es-noshorthands]{babel}
>>>>>>> 52e04f0 (feat: update models comparation)

% Use a new page for each section
\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

% Margenes
\usepackage{geometry}
\geometry{
  letterpaper,
  top=4cm,
  bottom=3cm,
  left=4cm,
  right=3cm
}

% Fuente
\usepackage{fontspec}
\setmainfont{Times New Roman}
<<<<<<< HEAD

\usepackage[
    backend=biber,
    style=apa,
    doi=true,
    sorting=nyt
]{biblatex}
\addbibresource{biblio.bib}

\usepackage[final]{microtype}
\setlength\emergencystretch{3em} % Emergency margin

% CODE
\usepackage{minted}
\setminted{fontsize=\footnotesize, breaklines, frame=lines}

\renewcommand{\baselinestretch}{1.5}

% Tables
\usepackage{booktabs}


% === Cover variables ===
\newcommand{\degreeTitle}{INGENIERO CIVIL INDUSTRIAL, MENCIÓN GESTIÓN}
\newcommand{\alumno}{Richard Peña Bonifaz}
\newcommand{\profPatro}{Manuel Saldaña Pino}
\newcommand{\profColab}{Luis Araya Alcázar}
\newcommand{\city}{Antofagasta}
\newcommand{\yearCover}{2025}

\newcommand{\ttitle}{Análisis Descriptivo y Predictivo Basado en Datos del Tráfico Vehicular en Antofagasta: Un Enfoque a partir de Reportes de Conductores}
\newcommand{\auth}{Richard Peña Bonifaz}

\usepackage{graphicx}
\usepackage{amsmath}

\usepackage{xurl}
\usepackage{hyperref}
\hypersetup{
    breaklinks=true,
    pdftitle={\ttitle},
    pdfauthor={\auth},
}
=======
\usepackage{csquotes}

\renewcommand{\baselinestretch}{1.5}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
>>>>>>> 52e04f0 (feat: update models comparation)
\usepackage{float}
\usepackage{parskip}
\usepackage[spanish]{cleveref}

\usepackage{algorithm}
<<<<<<< HEAD

% For hyperref the name to use \autoref should be
% {name of element}autorefname
\newcommand{\algorithmautorefname}{Algoritmo}

% Spanish
\floatname{algorithm}{Algoritmo}


=======
>>>>>>> 52e04f0 (feat: update models comparation)
\usepackage{algpseudocode}

\usepackage[caption = true]{subfig}

<<<<<<< HEAD
\usepackage[autostyle]{csquotes}

\begin{document}

\pagenumbering{gobble}
\input{cover}
=======
\usepackage{natbib}
\bibliographystyle{apalike}

\begin{document}

\title{Análisis Descriptivo y Predictivo Basado en Datos del Tráfico Vehicular en Antofagasta: Un Enfoque a partir de Reportes de Conductores}
\author{Richard Peña Bonifaz}
\date{\today}
\maketitle
>>>>>>> 52e04f0 (feat: update models comparation)

\section*{Dedicatoria}

A todas las personas que estuvieron conmigo y me alentaron en los momentos más difíciles.

\section*{Reconocimientos}

A los profesores apasionados por su trabajo, que no solo transmiten conocimiento, sino también la pasión por el saber y la aplicación de la materia que imparten.

\newpage
\tableofcontents
\listoffigures
\listoftables
\newpage

<<<<<<< HEAD
\section{Resumen}

\pagenumbering{arabic}
\setcounter{page}{1}

El proyecto tuvo como propósito desarrollar una herramienta para analizar y predecir el comportamiento del tráfico vehicular en la ciudad de Antofagasta, utilizando datos de la plataforma \textit{Waze for Cities}. Esta fuente, alimentada por su comunidad de usuarios, proporciona información en tiempo real sobre incidentes viales, permitiendo generar una visión detallada de las condiciones del tránsito urbano.

A partir del análisis de más de 53.000 eventos recopilados entre octubre de 2024 y abril de 2025, se identificaron patrones espaciales y temporales que evidencian zonas críticas y horarios de mayor riesgo. Se entrenaron tres modelos de clasificación —\textit{Random Forest}, \textit{Regresión Logística} y \textit{XGBoost}— seleccionándose este último por su mejor desempeño (\textbf{F1-score = 0.88}, \textbf{ROC–AUC = 0.84}).

Los resultados evidencian una asociación positiva y estadísticamente significativa entre congestión y accidentes ($r = 0.96$ en días laborales). En el marco de los modelos estimados, las variables de congestión muestran aporte predictivo para discriminar la ocurrencia de accidentes; sin embargo, esta asociación no implica causalidad y requiere validación con fuentes independientes. El sistema desarrollado integra estos modelos en un \textit{dashboard} web actualizado cada cinco minutos, disponible en \url{https://traficoantofagasta.com}, ofreciendo una herramienta de apoyo para la gestión del tráfico urbano y la toma de decisiones basadas en datos.
=======
\section*{Resumen}

El proyecto desarrollado tuvo como propósito la creación de una herramienta efectiva para analizar y predecir, con alta confiabilidad, el comportamiento del tráfico vehicular en la ciudad de Antofagasta, utilizando datos provenientes de la plataforma Waze Cities \citep{wazecities2024}. Esta plataforma, alimentada por su comunidad de usuarios, proporciona información en tiempo real que permite obtener una visión detallada de los eventos de tráfico en la ciudad. La investigación resultante generó información relevante para la gestión del tráfico, facilitando la toma de decisiones por parte de las autoridades locales, con miras a mejorar la seguridad vial y optimizar la eficiencia del flujo vehicular. A partir del análisis y explotación de estos datos, fue posible identificar patrones y tendencias que, al ser integrados en la planificación urbana, permiten optimizar rutas críticas, reducir la congestión y disminuir la probabilidad de accidentes.

Se consiguió obtener un modelo capaz de predecir el comportamiento del tráfico con una precisión del 85.83\% y un F1-score de 88.23\%, de acuerdo con el análisis de bondad de ajuste realizado al modelo final, el cual fue entrenado utilizando el algoritmo de clasificación XGBoost. Se utilizaron 64,708 eventos para el entrenamiento.
>>>>>>> 52e04f0 (feat: update models comparation)

\section{Introducción}
\subsection{Descripción del problema}

<<<<<<< HEAD
Antofagasta, una ciudad con más de 106,000 vehículos en circulación \parencite{conaset2023}, enfrenta desafíos significativos en la gestión de su tráfico vehicular. Durante el año 2023, se registraron 1,715 accidentes, los cuales resultaron en 31 fallecidos y 102 heridos graves \parencite{conaset2023}. Tuvo un crecimiento de su parque automotor de 0.8\% y 3.5\% en los años 2022 y 2023 respectivamente según datos del INE \parencite{ine2023}.

La infraestructura vial de Antofagasta se articula principalmente en torno a dos arterias que permiten atravesar la ciudad, la Avenida Edmundo Pérez Zujovic y la Avenida Pedro Aguirre Cerda, la segunda solo está presente en una parte del recorrido, por lo que para el resto, se deben utilizar diferentes alternativas en similares alturas de la ciudad, ninguna de estas continuas. Esto posiciona a la Avenida Edmundo Pérez Zujovic, la cual en el sector sur de la ciudad se encuentra con Avenida Grecia y Avenida Ejército, como la única vía continua que atraviesa la ciudad completamente. Esta particularidad dada las características geográficas de Antofagasta sumada a la alta concentración de vehículos, genera una alta congestión y  riesgo de accidentes, especialmente en las zonas anteriormente mencionadas.

Actualmente, no existe algún sistema de monitoreo en tiempo real que permitan gestionar el tráfico de manera proactiva. Los sistemas de monitoreo tradicionales, como son la gestión de semáforos y el estudio de vías, son valiosos para la administración del flujo vehicular y la estandarización de las intersecciones. Sin embargo, tiene limitaciones por ser estático para la vía en donde el diseño fue realizado. Este tipo de gestión tiene la limitación de no permitir una visión global del flujo vehicular, como también la poca flexibilidad ante los cambios que se generan en el entorno \parencite{auld2009}.

Explorar nuevas fuentes de datos, como los eventos vehiculares en la ciudad, o la gestión a través de visión por computadora, son herramientas que permiten gestionar de forma eficiente y efectiva el flujo vehicular, proporcionando datos en tiempo real, también ofrecen opciones de automatización y predictibilidad \parencite{chen2015}. Contar con grandes volúmenes de datos permite desarrollar modelos inteligentes para la gestión del tráfico vehicular.

Waze recopila datos de eventos reportados por los usuarios, los cuales cuentan con tres aspectos principales; el tiempo, la ubicación, y el tipo de evento. Esta categorización permite poder desarrollar modelos que detecten patrones y puedan estimar la probabilidad de eventos futuros, basándose en datos del pasado. Esta opción entrega una visión global de la ciudad, para poder detectar focos de atención en cuanto a la ocurrencia de accidentes y congestión vehicular. El uso de datos colaborativos, como los reportados por usuarios en Waze, ha demostrado ser una fuente válida para el análisis y predicción de patrones de tráfico urbano, permitiendo detectar focos críticos de congestión y accidentes \parencite{ferreira2017waze}.

La plataforma Waze Cities, permite a ciudades poder obtener datos para gestionar el tráfico vehicular con herramientas inteligentes, esto conlleva a mejorar la seguridad y la eficiencia en el tráfico vehicular, usando una fuente de dato ya existente \parencite{wazecitiescasestudies2024}.

\subsection{Metodología}

Se realiza un análisis geoespacial con el objetivo de identificar puntos críticos, como vías principales, calles secundarias y zonas de alto tráfico. Este análisis se llevó a cabo utilizando \texttt{GeoPandas}. Los resultados se presentan mediante técnicas de visualización que permiten interpretar las tendencias y puntos de interés de manera efectiva.

El pipeline de datos incluye una base de datos relacional para almacenarlos, un flujo ETL (Extract, Transform, Load) para procesarlos y un cliente web para visualizarlos. Se utilizó PostgreSQL \parencite{postgres2025} como base de datos SQL, Memcached  \parencite{memcached2025} como base de datos de caché, APScheduler para la programación de tareas y Dash \parencite{dash2025} como herramienta de visualización. Este enfoque permitió automatizar el flujo de datos y garantizar la actualización constante de la información.

Para el proceso de entrenamiento del modelo, se llevó a cabo un balanceo de clases, debido a la naturaleza de los datos, solo contiene datos de la ocurrencia de eventos, esto genera sesgos en el modelo y un sobre-ajuste que hace que el modelo tenga un rendimiento disminuido \parencite{he2009learning}.

Adicionalmente, se entrenó un modelo de clasificación para determinar la probabilidad de ocurrencia de accidentes en diferentes puntos de la ciudad. El modelo seleccionado fue XGBoost \parencite{chen2016xgboost}, el cual fue seleccionado utilizando técnicas de validación cruzada y optimización de hiperparámetros \parencite{geron2019hands}. Para la selección de variables se utilizó GridSearchCV \parencite{pedregosa2011scikit, geron2019hands}, una técnica de búsqueda de hiperparámetros que permite encontrar la mejor combinación de variables para el modelo, se compararon modelos de regresión logística, árboles de decisión y XGBoost.

Para la gestión del modelo, en cuanto a su implementación, mantenimiento y actualización, se programó APSCheduler, para ejecutar tareas periódicas que permiten gestionar el ciclo de vida de los modelos de aprendizaje automático, desde su entrenamiento hasta su despliegue en producción y versionado.

El servidor se desarrolló en \texttt{Rust} \parencite{rust2025}\footnote{Rust es un lenguaje de programación enfocado en seguridad de memoria y concurrencia sin necesidad de garbage collector, características deseables en sistemas en tiempo real.}. Se utilizó \texttt{Memcached} para generar un caché de los datos y de esta forma poder servirlos de manera más eficiente y rápida \parencite{fitzpatrick2004memcached}, debido al volumen de datos, se priorizó la unicidad de los mismos, evitando generar copias, para no aumentar el uso de memoria.

Para el procesado de los datos en el cliente —\texttt{Dash}—, se generan múltiples workers que permiten servir la aplicación de forma eficiente, y se utiliza solo una instancia en memoria de los datos, compartida por todos los workers. Para mantener la integridad de los datos, se implementa un \texttt{Mutex} para controlar el acceso de lectura a los mismos desde las tareas asíncronas \parencite{ramalho2015fluent}.

La perspectiva general del flujo de datos se muestra en la \autoref{fig:fuente} y el flujo desde la API de Waze hasta el dashboard se puede observar en la \autoref{fig:wf_dash}
=======
Antofagasta, una ciudad con más de 106,000 vehículos en circulación \citep{comision2023}, enfrenta desafíos significativos en la gestión de su tráfico vehicular. Durante el año 2023, se registraron 1,715 accidentes, los cuales resultaron en 31 fallecidos y 102 heridos graves \citep{comision2023}. Tuvo un crecimiento de su parque automotriz de 0.8\% y 3.5\% en los años 2022 y 2023 respectivamente según datos del INE \citep{ine2023}.

La infraestructura vial limitada cuenta con dos arterias principales para poder atravesar la ciudad, la Avenida Edmundo Perez Zujovic y la Avenida Pedro Aguirre Cerda, La segunda solo presente en la mitad del recorrido, por lo que para la otra mitad, se deben utilizar diferentes alternativas en similares alturas de la ciudad, ninguna de estas continuas. Esto posiciona a la Avenida Edmundo Perez Zujovic, la cual en el sector sur de la ciudad se encuentra con Avenida Grecia y Avenida Ejército, como la única vía continua que atraviesa la ciudad completamente. Esta particularidad dada las características geográficas de Antofagasta sumada a la alta concentración de vehículos, genera una alta congestión y  riesgo de accidentes, especialmente en las zonas anteriormente mencionadas.

Actualmente, no existe algún sistema de monitoreo en tiempo real que permitan gestionar el tráfico de manera proactiva. Los sistemas de monitoreo tradicionales, como son la gestión de semáforos y el estudio de vías, es valioso para la administración del flujo vehicular y la estandarización de las intersecciones. Sin embargo, tiene limitaciones por ser estático para la vía en donde el diseño fue realizado. Este tipo de gestión tiene la limitación de no permitir una visión global del flujo vehicular, como también la poca flexibilidad ante los cambios que se generan en el entorno \citep{auld2009}.

Explorar nuevas fuentes de datos, como los eventos vehiculares en la ciudad, o la gestión a través de visión por computadora, son herramientas que permiten gestionar de forma eficiente y efectiva el flujo vehicular, proporcionando datos en tiempo real, también ofrecen opciones de automatización y predictibilidad \citep{chen2015}. Contar con grandes volúmenes de datos permite desarrollar modelos inteligentes para la gestión del tráfico vehicular.

Waze recopila datos de eventos reportados por los usuarios, los cuales cuentan con tres aspectos principales; el tiempo, la ubicación, y el tipo de evento. Esta categorización permite poder desarrollar modelos que detecten patrones y puedan estimar con alta probabilidad eventos futuros, basándose en los datos pasados. Esta opción entrega una visión global de la ciudad, para poder detectar focos de atención en cuanto a la ocurrencia de accidentes y la congestión vehicular. El uso de datos colaborativos, como los reportados por usuarios en Waze, ha demostrado ser una fuente válida para el análisis y predicción de patrones de tráfico urbano, permitiendo detectar focos críticos de congestión y accidentes \citep{ferreira2017waze}.

La plataforma Waze Cities, permite a ciudades poder obtener datos para gestionar el tráfico vehicular con herramientas inteligentes, esto conlleva a mejorar la seguridad y la eficiencia en el tráfico vehicular, usando una fuente de dato ya existente \citep{wazecitiescasestudies2024}.

\subsection{Metodología}

Se realiza un análisis geoespacial con el objetivo de identificar puntos críticos, como vías principales, calles secundarias y zonas de alto tráfico. Este análisis se llevó a cabo utilizando \texttt{GeoPandas}. Los resultados se presentan mediante técnicas de visualización que permitien interpretar las tendencias y puntos de interés de manera efectiva.

El pipeline de datos incluye una base de datos relacional para almacenarlos, un flujo ETL (Extract, Transform, Load) para procesarlos y un servidor web para visualizarlos. Se utilizó PostgreSQL \citep{postgres2025} como base de datos, Memcached  \citep{memcached2025} como base de datos de caché, APScheduler para la programación de tareas y Dash \citep{dash2025} como herramienta de visualización. Este enfoque permitió automatizar el flujo de datos y garantizar la actualización constante de la información.

Adicionalmente, se entrenó un modelo de clasificación para determinar la probabilidad de ocurrencia de accidentes en diferentes puntos de la ciudad. El modelo seleccionado fue XGBoost \citep{chen2016xgboost}, el cual fue seleccionado utilizando técnicas de validación cruzada y optimización de hiperparámetros \citep{geron2019hands}. Para la selección de variables se utilizó GridSearchCV \citep{pedregosa2011scikit, geron2019hands}, una técnica de búsqueda de hiperparámetros que permite encontrar la mejor combinación de variables para el modelo, se compararon modelos de regresión logística, árboles de decisión y XGBoost.

Para la gestión del modelo, en cuanto a su implementación, mantenimiento y actualización, se programó APSCheduler, para ejecutar tareas periódicas que permiten gestionar el ciclo de vida de los modelos de aprendizaje automático, desde su entrenamiento hasta su despliegue en producción y versionado.

El servidor se desarrolló en \texttt{Rust} \citep{rust2025}, un lenguaje de programación enfocado en la eficiencia en el uso de memoria y seguridad en el uso de la misma. Se utilizó \texttt{Memcached} para generar un caché de los datos y de esta forma poder servirlos de manera más eficiente y rápida \citep{fitzpatrick2004memcached}, debido al volumen de datos, se priorizó la unicidad de los mismos, evitando generar copias, para no aumentar el uso de memoria.


Para el procesado de los datos en el cliente —\texttt{Dash}—, se generan múltiples workers que permiten servir la aplicación de forma eficiente, y se utiliza solo una instancia en memoria de los datos, compartidos por todos los datos, y para mantener la integridad de los datos, se implementa un \texttt{Mutex} para controlar el acceso de lectura a los mismos desde las tareas asíncronas \citep{ramalho2015fluent}.

La perspectiva general del flujo de datos se muestra en la \cref{fig:fuente} y el flujo desde la API de Waze hasta el dashboard se puede observar en la \cref{fig:wf_dash}
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/fuente_datos.png}
    \caption{Pipeline general de datos}
    \label{fig:fuente}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/wf_dash.png}
    \caption{Flujo de información en dashboard}
    \label{fig:wf_dash}
\end{figure}


\subsection{Objetivo general}

Analizar y modelar el comportamiento del tráfico vehicular en Antofagasta mediante datos de la plataforma Waze, con el fin de desarrollar una herramienta predictiva que contribuya a la gestión eficiente y segura del tráfico urbano.

\subsection{Objetivos específicos}

\begin{itemize}
    \item Realizar un análisis descriptivo de los datos recolectados para identificar patrones y tendencias relevantes en el comportamiento del tráfico.
<<<<<<< HEAD
    \item Presentar información visualmente comprensible y útil para la gestión vial, facilitando la implementación de políticas y acciones basadas en datos.
=======
    \item Presentar información visualmente comprensible y útil para las autoridades de gestión vial, facilitando la implementación de políticas y acciones basadas en datos.
>>>>>>> 52e04f0 (feat: update models comparation)
    \item Proponer una herramienta exploratoria para apoyar la toma de decisiones en la gestión vial
\end{itemize}

\subsection{Alcances}

Los datos utilizados corresponden al periodo desde octubre de 2024 hasta abril de 2025, en el sector urbano de la ciudad de Antofagasta, desde la zona norte urbana actual, sector Costa Laguna, hasta la zona sur, salida a camino de la minería, excluyendo el camino a puerto Coloso.

El área de estudio está delimitada por las coordenadas geográficas 23.35°S a 23.65°S de latitud y 70.36°O a 70.45°O de longitud, correspondientes al sistema de referencia EPSG:4326 (WGS84).

\section{Marco teórico}

<<<<<<< HEAD
El tráfico vehicular en entornos urbanos presenta un comportamiento complejo e impredecible, lo que dificulta su gestión eficiente. No obstante, el avance de las tecnologías móviles y la popularidad de aplicaciones como Waze permiten disponer de datos en tiempo real generados por los propios usuarios. Este proyecto se apoya en técnicas de análisis de datos y aprendizaje de máquinas (Machine Learning) para convertir esta información en herramientas útiles para la gestión vial. La utilización de datos geoespaciales, junto con la automatización de los procesos de recolección, análisis y visualización, constituye una solución costo-efectiva para mejorar la planificación del tráfico \parencite{barcelo2005}.

\subsection{Gestión de la seguridad vial}

Existen diferentes estrategias para la gestión vehicular, desde sistemas manuales como la dirección del tránsito por carabineros; o más automatizados, como la gestión inteligente de semáforos. En términos de seguridad, para evitar la ocurrencia de accidentes, resulta conveniente aplicar la jerarquía de control de riesgo (\autoref{fig:jerarquia_riesgos}). Esta busca abordar desde la medida de mayor impacto, hasta la de menor impacto en la exposición al riesgo. Se divide en dos grupos principales, las barreras duras y las barreras blandas (o administrativas). El primer grupo consta de la eliminación del agente de riesgo, la sustitución por uno de menor exposición o medidas ingenieriles, que buscan administrar la exposición con algún rediseño u organización. Las medidas administrativas constan de diferentes estrategias que puedan cambiar el comportamiento, el cuidado o la visibilidad de la exposición, por último, los elementos de protección personal, que buscan mitigar el daño ante algún posible accidente \parencite{niosh2024}.
=======
El tráfico vehicular en entornos urbanos presenta un comportamiento complejo e impredecible, lo que dificulta su gestión eficiente. No obstante, el avance de las tecnologías móviles y la popularidad de aplicaciones como Waze permiten disponer de datos en tiempo real generados por los propios usuarios. Este proyecto se apoya en técnicas de análisis de datos y aprendizaje de máquinas (Machine Learning) para convertir esta información en herramientas útiles para la gestión vial. La utilización de datos geoespaciales, junto con la automatización de los procesos de recolección, análisis y visualización, constituye una solución costo-efectiva para mejorar la planificación del tráfico \citep{barcelo2005}.

\subsection{Gestión de la seguridad vial}

Existen diferentes estrategias para la gestión vehicular, desde sistemas manuales como la dirección del tránsito por carabineros, o más automatizados, como la gestión inteligente de semáforos. En términos de seguridad, para evitar la ocurrencia de accidentes, resulta conveniente aplicar la jerarquía de control de riesgo \cref{fig:jerarquia_riesgos}. Esta busca abordar desde la medida de mayor impacto, hasta la de menor impacto en la exposición al riesgo. Se divide en dos grupos principales, las barreras duras y las barreras blandas (o administrativas). El primer grupo consta de la eliminación del agente de riesgo, la sustitución por uno de menor exposición o medidas ingenieriles, que buscan administrar la exposición con algún rediseño u organización. Las medidas administrativas constan de diferentes estrategias que puedan cambiar el comportamiento, el cuidado o la visibilidad de la exposición, por último, los elementos de protección personal, que buscan mitigar el daño ante algún posible accidente \citep{niosh2024}.
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/jerarquia_control_riesgo.png}
<<<<<<< HEAD
    \caption{Jerarquía de control de riesgos \parencite{niosh2024}.}
=======
    \caption{Jerarquía de control de riesgos. Fuente: \citet{niosh2024}.}
>>>>>>> 52e04f0 (feat: update models comparation)
    \label{fig:jerarquia_riesgos}
\end{figure}

La eliminación de la necesidad de transporte, puede ser abordada con estrategias como el fomento del teletrabajo para los puestos que lo permitan, o haciendo que las personas se encuentren más cerca de sus trabajos, eliminando la necesidad de transporte en vehículo. La sustitución se genera fomentando el uso de transporte público, el uso de medios alternativos de transporte, como las bicicletas o el uso compartido de vehículos. Las medidas de ingeniería buscan optimizar el flujo vehicular, puede ser a través del rediseño del plano urbano, el diseño de vehículos más seguros, monitoreo a través de radares, cámaras y GPS, planificación de rutas y nuevas vías de tránsito. Como medidas administrativas se encuentran la educación vial, la señalización y los límites de velocidad. En cuanto a elementos de protección personal, se posiciona el uso de cinturón de seguridad, la instalación de airbags, barreras anti-vuelco y barreras de contención en las vías, entre otras.

\subsection{Indicadores de bondad y ajuste}

<<<<<<< HEAD
Para evaluar la relación entre variables cuantitativas y validar los supuestos estadísticos de los modelos aplicados, se emplearon indicadores clásicos de ajuste y asociación. En particular, se utilizaron pruebas de hipótesis, el coeficiente de correlación de Pearson y el coeficiente de correlación de Spearman, herramientas fundamentales en el análisis estadístico inferencial \parencite{devore2011, montgomery2018, field2018}.

La \textbf{prueba de hipótesis} permite evaluar si existe suficiente evidencia en los datos para rechazar una afirmación nula sobre una población. En el caso de correlaciones, la hipótesis nula establece que no existe relación entre las variables (\( H_0 : \rho = 0 \)), mientras que la hipótesis alternativa plantea la existencia de una correlación (\( H_1 : \rho \neq 0 \)). El valor-p obtenido indica la probabilidad de observar un estadístico igual o más extremo bajo el supuesto de que la hipótesis nula es verdadera. Si el valor-p es inferior al nivel de significancia (\( \alpha \)), se rechaza la hipótesis nula \parencite{devore2011}.

El \textbf{coeficiente de correlación de Pearson} (\( r \)) mide la intensidad y dirección de la relación lineal entre dos variables cuantitativas. Su valor varía entre $-1$ y $1$, donde valores cercanos a $1$ indican una correlación positiva fuerte, valores cercanos a $-1$ una correlación negativa fuerte, y valores próximos a $0$ una relación débil o inexistente. Este indicador se define como:
=======
Para evaluar la relación entre variables cuantitativas y validar supuestos estadísticos, se utilizaron indicadores clásicos de ajuste y asociación. En particular, se aplicaron pruebas de hipótesis y el coeficiente de correlación de Pearson, herramientas fundamentales en el análisis estadístico inferencial \citep{devore2011, montgomery2012}.

La \textbf{prueba de hipótesis} permite evaluar si existe suficiente evidencia en los datos para rechazar una afirmación nula sobre una población. En el caso de correlaciones, la hipótesis nula establece que no existe relación lineal entre las variables (\( H_0 : \rho = 0 \)), mientras que la hipótesis alternativa plantea la existencia de una correlación (\( H_1 : \rho \neq 0 \)). El valor-p obtenido indica la probabilidad de observar un estadístico igual o más extremo, bajo el supuesto de que la hipótesis nula es verdadera. Si el valor-p es inferior al nivel de significancia (\( \alpha \)), se rechaza la hipótesis nula \citep{devore2011}.

El \textbf{coeficiente de correlación de Pearson} (\( r \)) mide la intensidad y dirección de la relación lineal entre dos variables cuantitativas. Su valor varía entre $-1$ y $1$, donde valores cercanos a $1$ indican una correlación positiva fuerte, valores cercanos a $-1$ una correlación negativa fuerte, y valores cercanos a $0$ una relación débil o inexistente. Este indicador viene dado por:
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{equation}
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2}}
\end{equation}

<<<<<<< HEAD
donde \( x_i \) y \( y_i \) representan los valores observados, y \( \bar{x} \) y \( \bar{y} \) sus respectivas medias \parencite{montgomery2018}.

No obstante, el coeficiente de Pearson asume linealidad y normalidad en las variables, condiciones que no siempre se cumplen en fenómenos reales como el tráfico vehicular. Por ello, se complementó el análisis mediante el estudio de los \textbf{residuos} del modelo, a fin de verificar la validez de los supuestos de normalidad, independencia y homocedasticidad. Los residuos, definidos como las diferencias entre los valores observados y los estimados, deben distribuirse aleatoriamente en torno a cero y sin patrones sistemáticos para garantizar que el modelo explique adecuadamente la relación entre las variables \parencite{montgomery2018}.

La evaluación de la normalidad de los residuos se realizó mediante la \textbf{prueba de D’Agostino–Pearson}, la cual combina medidas de asimetría y curtosis para contrastar la hipótesis de normalidad. Si el valor-p resultante es mayor que el nivel de significancia (\( \alpha = 0.05 \)), no se rechaza la hipótesis nula, lo que indica que los residuos se distribuyen aproximadamente de forma normal y las desviaciones del modelo pueden considerarse aleatorias.

De forma complementaria, se incorporó el \textbf{coeficiente de correlación de Spearman} (\( \rho_s \)), una medida no paramétrica que evalúa la relación monótona entre dos variables a partir del orden de sus valores, sin requerir supuestos de normalidad ni homocedasticidad \parencite{field2018}. Este coeficiente se define como la correlación de Pearson aplicada sobre los rangos de las observaciones:

\begin{equation}
\rho_s = 1 - \frac{6 \sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}
\end{equation}

donde \( d_i \) representa la diferencia entre los rangos de cada par de observaciones y \( n \) el número total de pares analizados. Al basarse en rangos, esta medida es más robusta frente a valores atípicos y distribuciones no normales, siendo especialmente útil cuando la relación entre las variables es creciente o decreciente, pero no estrictamente lineal.

Finalmente, para evaluar el ajuste de los modelos de regresión —tanto lineales como no lineales (exponencial inverso)— se emplearon indicadores de \textbf{bondad de ajuste}, como el coeficiente de determinación (\( R^2 \)) y el error cuadrático medio (ECM). El primero mide la proporción de la variabilidad explicada por el modelo, mientras que el segundo cuantifica la magnitud promedio del error entre los valores observados y los predichos. En conjunto, estas métricas permiten analizar el grado de precisión del modelo y su capacidad de generalización.

En síntesis, el enfoque adoptado en este estudio combina métodos \textbf{paramétricos} y \textbf{no paramétricos}, junto con un análisis exhaustivo de residuos, para garantizar la validez estadística y la robustez de las conclusiones. Este marco metodológico mixto permite que la inferencia sobre la relación entre congestión y accidentes se base tanto en fundamentos teóricos como en evidencia empírica contrastada.

\subsection{Ciencia de datos para el análisis de comportamiento}

Entender el comportamiento del tráfico vehicular, permite desarrollar medidas ingenieriles que disminuyan la probabilidad de accidentes, gestionando diversas variables como la congestión vehicular, los horarios con mayor probabilidad de accidentes y los sectores con mayor frecuencia de accidentes. La ciencia de datos permite, a través del uso de diferentes herramientas estadísticas, predecir con un grado de certeza conocido, la probabilidad de que ocurra un accidente tomando en cuenta variables que inciden en su ocurrencia.

Usando datos de eventos pasados, es posible generar modelos que, a partir de una entrada, entreguen una salida determinada según ciertos parámetros, esto se conoce como análisis predictivo. El análisis predictivo consiste en entrenar modelos con datos históricos para anticipar resultados futuros, utilizando distintas técnicas de aprendizaje automático, las cuales pueden clasificarse en aprendizaje supervisado, no supervisado y por refuerzo \parencite{murphy2012machine, geron2019hands, bishop2006pattern}.
=======
donde \( x_i \) y \( y_i \) representan los valores observados, y \( \bar{x} \) y \( \bar{y} \) sus respectivas medias \citep{montgomery2012}.

\subsection{Ciencia de datos para el análisis de comportamiento}

Entender el comportamiento del tráfico vehicular, permite desarrollar medidas ingenieriles que disminuyan la probabilidad de accidentes, gestionando diversas variables como la congestión vehicular, los horarios con mayor probabilidad de accidentes y los sectores con mayor frecuencia de accidentes. La ciencia de datos permite, a través del uso de diferentes herramientas estadísticas, predecir con un grado de certeza conocido, la probabilidad de que ocurra un accidente tomando en cuenta variables que puedan ser determinantes para la ocurrencia de los mismos.

Usando datos de eventos pasados, es posible generar modelos que, a partir de una entrada, entreguen una salida determinada según ciertos parámetros, esto se conoce como análisis predictivo. El análisis predictivo consiste en entrenar modelos con datos históricos para anticipar resultados futuros, utilizando distintas técnicas de aprendizaje automático, las cuales pueden clasificarse en aprendizaje supervisado, no supervisado y por refuerzo \citep{murphy2012machine, geron2019hands, bishop2006pattern}.
>>>>>>> 52e04f0 (feat: update models comparation)

En el aprendizaje supervisado, el modelo recibe tanto los datos de entrada como los de salida esperada, lo que le permite aprender patrones y realizar predicciones sobre nuevos datos con base en ese entrenamiento. El aprendizaje no supervisado utiliza datos de entrada, buscando estructuras o patrones en los mismos sin una clasificación específica, dejando la interpretación a cargo del especialista. Finalmente, en el aprendizaje por refuerzo, el modelo recibe una entrada, ejecuta una acción o predicción, y ajusta su comportamiento en función de una retroalimentación positiva o negativa, con el objetivo de maximizar el rendimiento a lo largo del tiempo.

Adicionalmente, los modelos pueden clasificarse según el tipo de salida que generan: modelos de regresión, que entregan valores numéricos continuos, y modelos de clasificación, que asignan las observaciones a una o más categorías previamente definidas. En estos últimos, el resultado se expresa como una distribución de probabilidades, y se considera como salida final la categoría con mayor probabilidad.

En este estudio se empleó un modelo supervisado de clasificación, con el objetivo de estimar la probabilidad de ocurrencia de un accidente, considerando dos posibles resultados: “ocurre” o “no ocurre”.

<<<<<<< HEAD
\subsection{Evaluación de los modelos de Machine Learning}

Para evaluar el rendimiento del modelo se utilizaron métricas ampliamente empleadas en problemas de clasificación binaria, tales como la exactitud (\textit{accuracy}), la precisión (\textit{precision}), la recuperación (\textit{recall}) y la medida F1 (\textit{F1-score}) \parencite{manning2008}.
La exactitud representa el porcentaje total de predicciones correctas sobre el total de casos evaluados.
La precisión corresponde a la proporción de casos positivos correctamente identificados por el modelo, es decir, el número de verdaderos positivos dividido por el total de predicciones positivas.
Por su parte, la recuperación —también conocida como \textit{recall}— mide la capacidad del modelo para identificar correctamente todos los casos positivos reales, dividiendo los verdaderos positivos por el total de casos positivos reales.
La medida F1, en tanto, es la media armónica entre precisión y recuperación, y proporciona una visión equilibrada del desempeño del modelo cuando es necesario considerar tanto los errores por omisión como los errores por comisión.

Además, se construyó una \textbf{matriz de confusión}, la cual permite observar el número de aciertos y errores cometidos en cada clase —“ocurre” y “no ocurre”—, facilitando una evaluación más detallada del comportamiento del modelo.
Este análisis es clave para comprender sus limitaciones y fortalezas, especialmente en aplicaciones reales relacionadas con la predicción de accidentes de tránsito.
=======
Para evaluar el rendimiento del modelo se utilizaron métricas ampliamente empleadas en problemas de clasificación binaria, tales como la exactitud (\textit{accuracy}), la precisión (\textit{precision}), la sensibilidad (\textit{recall}) y la medida F1 (\textit{F1-score}) \citep{manning2008}. La exactitud representa el porcentaje total de predicciones correctas sobre el total de casos evaluados. La precisión corresponde a la proporción de casos positivos correctamente identificados por el modelo, es decir, el número de verdaderos positivos dividido por el total de predicciones positivas. Por su parte, la sensibilidad —también conocida como \textit{recall}— mide la capacidad del modelo para identificar correctamente todos los casos positivos reales, dividiendo los verdaderos positivos por el total de casos positivos reales. La medida F1, en tanto, es la media armónica entre precisión y sensibilidad, y proporciona una visión equilibrada del desempeño del modelo cuando es necesario considerar tanto los errores por omisión como los errores por comisión.

Además, se construyó una matriz de confusión, la cual permite observar el número de aciertos y errores cometidos en cada clase —“ocurre” y “no ocurre”— lo que facilita una evaluación más detallada del comportamiento del modelo. Este análisis es clave para comprender sus limitaciones y fortalezas, especialmente en aplicaciones reales relacionadas con la predicción de accidentes de tránsito.
>>>>>>> 52e04f0 (feat: update models comparation)

Las fórmulas utilizadas para el cálculo de estas métricas son las siguientes:

\[
\begin{aligned}
\text{Exactitud (Accuracy)} &= \frac{TP + TN}{TP + TN + FP + FN} \\
\text{Precisión (Precision)} &= \frac{TP}{TP + FP} \\
<<<<<<< HEAD
\text{Recuperación (Recall)} &= \frac{TP}{TP + FN} \\
\text{Medida F1 (F1-score)} &= 2 \cdot \frac{\text{Precisión} \cdot \text{Recuperación}}{\text{Precisión} + \text{Recuperación}}
=======
\text{Sensibilidad (Recall)} &= \frac{TP}{TP + FN} \\
\text{Medida F1 (F1-score)} &= 2 \cdot \frac{\text{Precisión} \cdot \text{Sensibilidad}}{\text{Precisión} + \text{Sensibilidad}}
>>>>>>> 52e04f0 (feat: update models comparation)
\end{aligned}
\]

donde \( TP \) son los verdaderos positivos, \( TN \) los verdaderos negativos, \( FP \) los falsos positivos y \( FN \) los falsos negativos.

<<<<<<< HEAD
\vspace{1em}

Adicionalmente, se emplearon indicadores basados en curvas de rendimiento que permiten analizar el comportamiento del clasificador a distintos umbrales de decisión.
Entre ellas destacan la \textbf{curva ROC} (\textit{Receiver Operating Characteristic}) y la \textbf{curva Precision–Recall}, junto con sus respectivas áreas bajo la curva (\textbf{AUC}, por sus siglas en inglés).

La \textbf{curva ROC} representa la relación entre la \textit{tasa de verdaderos positivos} (TPR o \textit{recall}) y la \textit{tasa de falsos positivos} (FPR) a medida que varía el umbral de decisión.
El \textbf{área bajo la curva ROC (ROC–AUC)} cuantifica la capacidad global del modelo para discriminar entre clases; un valor cercano a 1 indica un clasificador con excelente poder de separación, mientras que un valor de 0.5 equivale a una predicción aleatoria \parencite{fawcett2006roc}.

Por su parte, la \textbf{curva Precision–Recall} describe la relación entre la \textit{precisión} y la \textit{recuperación} del modelo, siendo especialmente útil en contextos con clases desbalanceadas, donde la curva ROC puede ofrecer una visión demasiado optimista del desempeño \parencite{saito2015}.
El \textbf{área bajo la curva Precision–Recall (PR–AUC)} resume este equilibrio: valores elevados reflejan una mayor proporción de predicciones positivas correctas sin pérdida significativa de sensibilidad.

El análisis conjunto de ROC–AUC y PR–AUC permite evaluar tanto la capacidad discriminativa del modelo como su utilidad práctica en escenarios donde los falsos positivos y falsos negativos tienen costos distintos, como ocurre en la predicción de eventos de tráfico.

Finalmente, se consideró la \textbf{curva de aprendizaje} (\textit{learning curve}) como herramienta de diagnóstico para analizar el comportamiento del modelo durante el entrenamiento.
Esta curva muestra cómo varía el rendimiento del modelo —medido por la exactitud o el error— en función del tamaño del conjunto de entrenamiento, manteniendo constante el conjunto de validación \parencite{raschka2018}.

El análisis de la curva de aprendizaje permite identificar problemas de \textbf{sobreajuste} (\textit{overfitting}) o \textbf{subajuste} (\textit{underfitting}).
Cuando el desempeño en entrenamiento es alto pero bajo en validación, el modelo está sobreajustando a los datos, mientras que si ambos son bajos, el modelo presenta subajuste.
Por el contrario, cuando las curvas de entrenamiento y validación convergen hacia un valor alto y estable, se considera que el modelo ha alcanzado una buena capacidad de generalización.
En este estudio, la curva de aprendizaje se utilizó para verificar la estabilidad de los modelos entrenados y determinar si la incorporación de más datos o ajustes de hiperparámetros podría mejorar el rendimiento sin incrementar la varianza del modelo.

\subsection{Algoritmos de clasificación: Random Forest, Logistic Regression y XGBoost}

Para abordar problemas de clasificación binaria, como la predicción de ocurrencia o no ocurrencia de eventos de tráfico, se utilizaron algoritmos de aprendizaje supervisado. En particular, se emplearon los métodos \textit{Random Forest}, \textit{Logistic Regression} y \textit{XGBoost}, los cuales han demostrado un alto rendimiento en tareas de clasificación estructurada \parencite{friedman2001elements, chen2016xgboost, hosmer2013applied}.

\textbf{Random Forest} es un algoritmo basado en el principio del ensamblado de modelos, específicamente mediante la construcción de múltiples árboles de decisión. Cada árbol es entrenado sobre una muestra aleatoria del conjunto de datos (con reemplazo), y en cada nodo del árbol se selecciona una subdivisión óptima a partir de un subconjunto aleatorio de características. Esta técnica, conocida como \textit{bagging} (bootstrap aggregating), permite reducir la varianza del modelo y mejorar su capacidad de generalización. La predicción final se obtiene mediante votación mayoritaria entre todos los árboles del conjunto \parencite{breiman2001random}.

\textbf{Logistic Regression} es uno de los modelos estadísticos más utilizados para tareas de clasificación binaria. Se basa en la estimación de la probabilidad de pertenencia a una clase mediante la función logística o sigmoide, que transforma una combinación lineal de las variables predictoras en valores comprendidos entre 0 y 1. Este enfoque permite interpretar los coeficientes del modelo como efectos marginales sobre la probabilidad de ocurrencia del evento, manteniendo una alta interpretabilidad y simplicidad computacional. No obstante, su desempeño puede verse limitado ante relaciones no lineales o interacciones complejas entre las variables \parencite{hosmer2013applied}.

\textbf{XGBoost} (\textit{Extreme Gradient Boosting}) es un algoritmo basado en el método de \textit{boosting}, donde los árboles son construidos de manera secuencial. A diferencia del \textit{bagging}, el \textit{boosting} busca corregir los errores cometidos por los modelos anteriores, ajustando cada nuevo árbol a los residuos del conjunto anterior. XGBoost optimiza una función de pérdida regularizada utilizando técnicas avanzadas como la poda previa, el manejo eficiente de valores faltantes y una implementación paralelizada, lo que lo convierte en uno de los algoritmos más eficientes y precisos en clasificación tabular \parencite{chen2016xgboost}.

Estos algoritmos permiten capturar relaciones tanto lineales como no lineales entre las variables predictoras y la variable objetivo, siendo adecuados para trabajar con datos heterogéneos, como los recopilados desde fuentes de tráfico urbano. En la etapa de desarrollo se aplicaron los modelos para comparar su desempeño predictivo sobre el conjunto de datos estudiado.

\subsection{Análisis geoespacial}

Waze entrega las coordenadas geoespaciales de los eventos de tráfico en el sistema de referencia \textbf{EPSG:4326}. Este sistema corresponde al datum geodésico global \textbf{WGS84 (World Geodetic System 1984)}, ampliamente utilizado en aplicaciones de posicionamiento global (GPS) y cartografía digital \parencite{epsg4326, wgs84_nima}. En EPSG:4326, las coordenadas están expresadas en grados decimales de latitud y longitud, lo cual es adecuado para visualización, pero no para realizar cálculos métricos directos como distancias o áreas.

Para llevar a cabo análisis espaciales cuantitativos sobre los eventos reportados por Waze —incluyendo cálculos de distancia, densidad y agrupación geográfica— fue necesario transformar dichas coordenadas a un sistema proyectado con unidades métricas.

La transformación se realizó utilizando la proyección Transversa de Mercator (Transverse Mercator), en su implementación como zona UTM 19 Sur (EPSG:32719). Esta proyección es conforme y está basada en el elipsoide WGS84, con un meridiano central en $\lambda_0 = -69^\circ$, un factor de escala $k_0 = 0.9996$, y un falso este de $500\,000$ m. Para el hemisferio sur, además, se aplica una corrección vertical de $10\,000\,000$ m a la coordenada norte. Esto se detalla en el \autoref{ap:utm}.

Esta transformación se aplicó mediante la función \texttt{to\_crs} de la librería \texttt{GeoPandas}, la cual utiliza internamente \texttt{Pyproj} como interfaz de la librería PROJ \parencite{gdal_proj, pyproj, geopandas}. La elección del sistema \textbf{EPSG:32719} permite minimizar la distorsión local, ya que el sistema UTM divide el globo en zonas longitudinales estrechas, optimizando la precisión en cada región. Al ser una proyección conforme y métrica, garantiza que las distancias, áreas y densidades calculadas sobre el plano proyectado sean válidas y consistentes con el Sistema Internacional de Unidades \parencite{epsg32719}.

\subsection{Aprendizaje automático}

A medida que la cantidad de datos aumenta, el modelo requiere ser reentrenado para incorporar los nuevos registros y así actualizar sus parámetros. Este proceso de aprendizaje continuo permite que el modelo se adapte a las variaciones que puedan presentarse en los patrones de tráfico vehicular a lo largo del tiempo. Para asegurar esta adaptabilidad, se programó una frecuencia de reentrenamiento mensual, lo que implica la incorporación de aproximadamente 9,244 nuevos datos al modelo cada mes. El reentrenamiento periódico resulta fundamental para mantener la vigencia y precisión del modelo, ya que permite integrar las tendencias más recientes y responder oportunamente a los cambios en las condiciones del tráfico \parencite{gama2014concept}.


Para la gestión de los modelos se utilizó \texttt{MLFlow}, un \texttt{framework} que permite gestionar las versiones de los modelos y almacenarlos en una base de datos, asegurando la persistencia de los mismos. También permite almacenar las diferentes métricas de cada versión del modelo \parencite{mlflow2025}.

Para la gestión del modelo, en cuanto a su implementación, mantenimiento y actualización, se programó con \texttt{APSCheduler}, una librería de tareas asíncronas en Python \parencite{apscheduler2025}.

\subsection{Investigaciones previas}

La literatura sobre análisis y predicción del tráfico urbano integra tres líneas principales: (i) \textit{fuentes de datos colaborativas y en tiempo real} para caracterizar el estado de la red; (ii) \textit{modelos predictivos} (estadísticos y de aprendizaje automático) para estimar riesgo/ocurrencia de incidentes; y (iii) \textit{métodos geo–espaciotemporales} para identificar y explicar patrones de concentración. De forma transversal, se estudian estrategias para enfrentar \textit{datos desbalanceados} y \textit{evaluación adecuada de clasificadores}.

\paragraph{Datos colaborativos y analítica en tiempo real.}
El uso de plataformas de reporte ciudadano (como Waze) y fuentes heterogéneas ha mostrado ser efectivo para caracterizar congestión y siniestralidad a escala urbana, permitiendo alimentar gemelos digitales y tableros operacionales con latencias bajas \parencite{chen2015, ferreira2017waze, wazecitiescasestudies2024}. Trabajos con simuladores meso/microscópicos (p.\,ej., AIMSUN) han servido para validar escenarios y políticas de control bajo supuestos realistas de demanda y oferta \parencite{barcelo2005}.

\paragraph{Modelos predictivos de incidentes.}
En predicción de accidentes y eventos de tráfico, se han utilizado modelos desde regresiones generalizadas (p.\,ej., binomial negativa para frecuencias) hasta \textit{ensemble} (Random Forest, Gradient Boosting/XGBoost) y redes neuronales, reportándose ventajas consistentes de los métodos no lineales frente a relaciones complejas entre variables \parencite{lord2010crashfreq, chen2016xgboost, vanlint2005, berhanu2024}. Dado el interés práctico por interpretabilidad, técnicas de explicación pos–hoc como \textit{SHAP} permiten descomponer la contribución de variables y favorecer decisiones de gestión con trazabilidad \parencite{lundberg2017shap}.

\paragraph{Análisis espacial y espaciotemporal.}
La identificación de \textit{hotspots} de siniestralidad se ha abordado mediante estimación de densidad por núcleo (KDE) en red vial y pruebas de autocorrelación espacial (Moran's I, LISA) y de clústeres locales (Getis–Ord $G_i^*$), así como agrupamiento espaciotemporal (ST–DBSCAN) para capturar persistencia temporal y proximidad espacial de incidentes \parencite{xie2013kde, moran1950, anselin1995lisa, getis1992, birant2007stdbscan}. Estas herramientas complementan la modelación predictiva al ofrecer una lectura geométrica de la exposición y permiten priorizar intervenciones.

\paragraph{Desbalance de clases y generación de no–eventos.}
En problemas de seguridad vial es común el desbalance positivo/negativo. La literatura recomienda tanto \textit{reponderación} de clases como \textit{resampling} (p.\,ej., SMOTE) y, cuando no existen registros negativos explícitos, la generación controlada de no–eventos a partir de la combinación de rejillas temporales y segmentación espacial, cuidando la consistencia con el proceso generativo \parencite{chawla2002smote, goedertier2009robust}. En este trabajo se adopta una estrategia 1:1 de eventos simulados negativos, coherente con enfoques previos para clasificar ocurrencia/no–ocurrencia bajo restricciones espaciotemporales.

\paragraph{Evaluación de clasificadores en escenarios desbalanceados.}
Además de la curva ROC–AUC, la literatura recomienda la curva Precision–Recall (PR–AUC) por su mayor sensibilidad a la clase positiva cuando es minoritaria \parencite{saito2015}. La comparación de modelos debe considerar ambas métricas y, cuando sea posible, análisis de \textit{calibración} de probabilidades y matrices de costo específicas del dominio.

En síntesis, el estado del arte respalda el uso de fuentes colaborativas, aprendizaje automático no lineal e instrumentos geo–espaciotemporales para comprender y predecir eventos de tráfico. Este estudio se alinea con dichas evidencias: integra datos colaborativos (Waze), compara algoritmos (Regresión Logística, Random Forest, XGBoost), aplica métodos espaciales y enfrenta el desbalance mediante no–eventos simulados y métricas apropiadas (ROC–AUC y PR–AUC).

\section{Desarrollo}

Se diseña una estrategia de recolección de datos que garantiza información con un nivel de certeza conocido. La API de la plataforma Waze Cities, que proporciona información en tiempo real sobre eventos activos, fue la fuente principal de datos. Para lograr un volumen de datos representativo, se implementó un servidor encargado de recopilar y almacenar esta información de manera continua. El servidor fue desarrollado en \texttt{Rust}, utilizando la librería reqwest \parencite{reqwest2025} para extraer los datos desde la API de la plataforma Waze Cities. Se recolectan los eventos de tráfico y se almacenan en una base de datos PostgreSQL. Este servidor se ejecuta de manera continua actualizando la información cada 2 minutos. La estructura de la base de datos se muestra en la \autoref{fig:db_diagram}.
=======
\subsection{Algoritmos de clasificación: Random Forest y XGBoost}

Para abordar problemas de clasificación binaria, como la predicción de ocurrencia o no ocurrencia de eventos de tráfico, se utilizaron algoritmos de aprendizaje supervisado. En particular, se emplearon los métodos \textit{Random Forest} y \textit{XGBoost}, los cuales han demostrado un alto rendimiento en tareas de clasificación estructurada \citep{friedman2001elements, chen2016xgboost}.

\textbf{Random Forest} es un algoritmo basado en el principio del ensamblado de modelos, específicamente mediante la construcción de múltiples árboles de decisión. Cada árbol es entrenado sobre una muestra aleatoria del conjunto de datos (con reemplazo), y en cada nodo del árbol se selecciona una subdivisión óptima a partir de un subconjunto aleatorio de características. Esta técnica, conocida como \textit{bagging} (bootstrap aggregating), permite reducir la varianza del modelo y mejorar su capacidad de generalización. La predicción final se obtiene mediante votación mayoritaria entre todos los árboles del conjunto \citep{breiman2001random}.

\textbf{XGBoost} (\textit{Extreme Gradient Boosting}), es un algoritmo basado en el método de \textit{boosting}, donde los árboles son construidos de manera secuencial. A diferencia del \textit{bagging}, el \textit{boosting} busca corregir los errores cometidos por los modelos anteriores, ajustando cada nuevo árbol a los residuos del conjunto anterior. XGBoost optimiza una función de pérdida regularizada utilizando técnicas avanzadas como la poda previa, el manejo eficiente de valores faltantes y una implementación paralelizada, lo que lo convierte en uno de los algoritmos más eficientes y precisos en clasificación tabular \citep{chen2016xgboost}.

Ambos algoritmos permiten capturar relaciones no lineales y complejas entre las variables predictoras y la variable objetivo, siendo adecuados para trabajar con datos heterogéneos, como los recopilados desde fuentes de tráfico urbano. En la etapa de desarrollo se aplicaron ambos modelos para comparar su desempeño predictivo sobre el conjunto de datos estudiado.

\subsection{Análisis geoespacial}

Waze entrega las coordenadas geoespaciales de los eventos de tráfico en el sistema de referencia \textbf{EPSG:4326}. Este sistema corresponde al datum geodésico global \textbf{WGS84 (World Geodetic System 1984)}, ampliamente utilizado en aplicaciones de posicionamiento global (GPS) y cartografía digital \citep{epsg4326, wgs84_nima}. En EPSG:4326, las coordenadas están expresadas en grados decimales de latitud y longitud, lo cual es adecuado para visualización, pero no para realizar cálculos métricos directos como distancias o áreas.

Para llevar a cabo análisis espaciales cuantitativos sobre los eventos reportados por Waze —incluyendo cálculos de distancia, densidad y agrupación geográfica— fue necesario transformar dichas coordenadas a un sistema proyectado con unidades métricas.

La transformación se realizó utilizando la proyección Transversa de Mercator (Transverse Mercator), en su implementación como zona UTM 19 Sur (EPSG:32719). Esta proyección es conforme y está basada en el elipsoide WGS84, con un meridiano central en $\lambda_0 = -69^\circ$, un factor de escala $k_0 = 0.9996$, y un falso este de $500\,000$ m. Para el hemisferio sur, además, se aplica una corrección vertical de $10\,000\,000$ m a la coordenada norte. Esto se detalla en el Apéndice~\ref{ap:utm}.

Esta transformación se aplicó mediante la función \texttt{to\_crs} de la librería \texttt{GeoPandas}, la cual utiliza internamente la librería \texttt{Pyproj} como interfaz con la librería PROJ \citep{gdal_proj, pyproj, geopandas}.

La elección de EPSG:32719 permite minimizar la distorsión local, ya que el sistema UTM divide el globo en zonas longitudinales estrechas, optimizando la precisión en cada región. Al ser una proyección conforme y métrica, asegura que las distancias, áreas y densidades calculadas sobre el plano proyectado sean válidas y consistentes con el Sistema Internacional de Unidades \citep{epsg32719}.

\subsection{Aprendizaje automático}

A medida que la cantidad de datos aumenta, el modelo requiere ser reentrenado para incorporar los nuevos registros y así actualizar sus parámetros. Este proceso de aprendizaje continuo permite que el modelo se adapte a las variaciones que puedan presentarse en los patrones de tráfico vehicular a lo largo del tiempo. Para asegurar esta adaptabilidad, se programó una frecuencia de reentrenamiento mensual, lo que implica la incorporación de aproximadamente 9,244 nuevos datos al modelo cada mes. El reentrenamiento periódico resulta fundamental para mantener la vigencia y precisión del modelo, ya que permite integrar las tendencias más recientes y responder oportunamente a los cambios en las condiciones del tráfico \citep{gama2014concept}.


Para la gestión de los modelos se utilizó \texttt{MLFlow}, un \texttt{framework} que permite gestionar las versiones de los modelos y almacenarlos en una base de datos, asegurando la persistencia de los mismos. También permite almacenar las diferentes métricas de cada versión del modelo \citep{mlflow2025}.

Para la gestión del modelo, en cuanto a su implementación, mantenimiento y actualización, se programó con \texttt{APSCheduler}, una librería de tareas asíncronas en Python \citep{apscheduler2025}.

\subsection{Investigaciones previas}

El análisis del tráfico vehicular y la predicción de accidentes han sido objeto de múltiples estudios en los últimos años. Por ejemplo, \citet{barcelo2005} exploraron simulaciones dinámicas de redes de tráfico utilizando AIMSUN, destacando su capacidad para modelar escenarios urbanos complejos. Por otro lado, \citet{vanlint2005} desarrollaron modelos basados en redes neuronales para predecir tiempos de viaje en autopistas, incluso en condiciones de datos faltantes, demostrando la utilidad de enfoques basados en aprendizaje automático.

En el ámbito de la gestión de tráfico basada en datos, \citet{chen2015} propusieron enfoques impulsados por datos para la predicción y gestión del tráfico, enfatizando la importancia de integrar fuentes de datos en tiempo real. Asimismo, \citet{goedertier2009robust} introdujeron técnicas para descubrir procesos robustos mediante eventos negativos artificiales, lo que permite abordar problemas de clasificación supervisada en contextos de tráfico.

Más recientemente, \citet{berhanu2024} aplicaron aprendizaje automático y análisis espacial para predecir accidentes de tráfico y optimizar rutas en redes urbanas congestionadas, destacando la relevancia de combinar modelos predictivos con análisis geoespacial.

\section{Desarrollo}

Se diseña una estrategia de recolección de datos que garantiza información con un nivel adecuado de certeza. La API de Waze Cities, que proporciona información en tiempo real sobre eventos activos, fue la fuente principal de datos. Para lograr un volumen de datos representativo, se implementó un servidor encargado de recopilar y almacenar esta información de manera continua. El servidor fue desarrollado en \texttt{Rust}, utilizando la librería reqwest \citep{reqwest2025} para extraer los datos desde la API de Waze Cities. Se recolectan los eventos de tráfico y se almacenan en una base de datos PostgreSQL. Este servidor se ejecuta de manera continua actualizando la información cada 2 minutos. La estructura de la base de datos se muestra en la \cref{fig:db_diagram}.
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/db_diagram.png}
    \caption{Diagrama de la base de datos}
    \label{fig:db_diagram}
\end{figure}

<<<<<<< HEAD
Existen dos estructuras principales, las alertas (\texttt{alerts}) y las congestiones (\texttt{jams}), el primero registra todos los eventos reportados por usuarios, tales como peligros, accidentes, congestión o rutas cerradas. En el caso de los eventos de congestión, son datos generados automáticamente por Waze, los cuales usando la geolocalización, estiman congestiones en diferentes puntos de la ciudad. Para este proyecto, se utiliza la información de las alertas. Sin embargo, se almacenaron los datos de congestión, para un posterior análisis, que está fuera del alcance de este estudio. Esta estructura se definió con base en los datos relevantes y completos que proporciona la API, los tipos de datos disponibles pueden ser consultados en la documentación de Waze \parencite{waze2024}.

Desde octubre de 2024 hasta abril de 2025, se capturan 53,159 alertas, que al ser filtradas (ver \autoref{ssec:filter}), finalmente se obtienen 26,336, las cuales se analizarán a lo largo de este estudio.


\subsection{Filtrado temporal y agrupación de eventos} \label{ssec:filter}

Las alertas pueden contener duplicados, debido a que diferentes usuarios pueden reportar el mismo evento. Para evitar esto, se filtraron los datos agrupándolos por segmento (\autoref{ssec:freq_zone}) y periodos de tiempo, eliminando grupos duplicados en una ventana de tiempo de 120 minutos, es decir, todo evento que se reportó con el mismo tipo, en el mismo segmento, dentro de un periodo de dos horas, será considerado como un solo evento. Este procedimiento, resumido en el \autoref{alg:filter}, permite homogeneizar la granularidad temporal de los eventos y asegurar la consistencia tipológica de los atributos (\texttt{group}, \texttt{type}). El código fuente completo se presenta en el \autoref{annex:filter_code}.

De este modo, el filtro actúa como un mecanismo de depuración de ruido observacional, preservando un único registro representativo por evento real. Aunque esta consolidación produce una ligera disminución en las métricas de desempeño (\texttt{ROC AUC} y \texttt{F1-score}) –alrededor de 4\%–, el efecto se considera metodológicamente favorable, ya que aumenta la consistencia temporal y evita el sesgo hacia zonas con mayor densidad de usuarios o reportes. En consecuencia, el modelo resultante refleja con mayor precisión la ocurrencia efectiva de incidentes de tráfico, en lugar de su frecuencia de reporte.


\begin{algorithm}[H]
\caption{Filtrado temporal y agrupación de eventos}
\label{alg:filter}
\begin{algorithmic}[1]
\Require Conjunto de datos $D$, tamaño de intervalo $\Delta t$ (en minutos)
\Ensure Conjunto filtrado y agrupado $D'$

\State $paso \gets \Delta t \times 60000$ \Comment{Conversión de minutos a milisegundos}

\ForAll{evento $\in D$}
    \If{\texttt{pub\_millis} no es entero}
        \State evento.\texttt{pub\_millis} $\gets$ Redondear(evento.\texttt{pub\_millis} / $10^6$)
    \EndIf
    \State evento.\texttt{interval\_start} $\gets \lfloor$evento.\texttt{pub\_millis} / paso$\rfloor \times$ paso
\EndFor

\State Convertir \texttt{interval\_start} a formato fecha-hora (UTC)
\State Asegurar tipo entero en \texttt{group} y tipo texto en \texttt{type}
\State Eliminar duplicados en [\texttt{interval\_start}, \texttt{group}, \texttt{type}]
\State Eliminar columna auxiliar \texttt{interval\_start}
\State Convertir \texttt{pub\_millis} a formato fecha-hora (zona local)
\State Reiniciar índices de $D$
\State \Return $D$
=======
Existen dos estructuras principales, las alertas (\texttt{alerts}) y las congestiones (\texttt{jams}), el primero registra todos los eventos reportados por usuarios, tales como peligros, accidentes, congestión o rutas cerradas. En el caso de los eventos de congestión, son datos generados automáticamente por Waze, los cuales usando la geolocalización, estiman congestiones en diferentes puntos de la ciudad. Para este proyecto, se utiliza la información de las alertas. Sin embargo, se almacenaron los datos de congestión, para un posterior análisis, que está fuera del alcance de este estudio. Esta estructura se definió con base en los datos relevantes y completos que proporciona la API, los tipos de datos disponibles pueden ser consultados en la documentación de Waze \citep{waze2024}.

Desde octubre de 2024 hasta abril de 2025, se capturan 32,354 alertas, las cuales se analizarán a lo largo de este estudio.

\subsection{Consideraciones de los datos}

Las alertas pueden contener duplicados, debido a que diferentes usuarios pueden reportar el mismo evento. Para evitar esto, se filtraron los datos agrupándolos por segmento (\cref{ssec:freq_zone}) y periodos de tiempo, eliminando grupos duplicados en una ventana de tiempo de 120 minutos, es decir, todo evento que se reportó con el mismo tipo, en el mismo segmento, dentro de un periodo de dos horas, será considerado como un solo evento. Para esto se utiliza un algoritmo de filtrado \cref{alg:filter}.

\begin{algorithm}
\caption{Filtrado de eventos por grupo y tiempo}
\label{alg:filter}
\begin{algorithmic}[1]
\Require datos\_entrada, intervalo\_minutos
\State datos\_filtrados $\gets \emptyset$

\If{datos\_entrada = $\emptyset$ \textbf{or} no existe pub\_millis}
    \Return datos\_filtrados
\EndIf

\State datos $\gets$ Copiar(datos\_entrada)
\State paso $\gets$ intervalo\_minutos $\times$ 60000 \Comment{Convertir a milisegundos}

\ForAll{evento \textbf{in} datos}
    \If{NO EsEntero(evento.pub\_millis)}
        \State evento.pub\_millis $\gets$ Redondear(evento.pub\_millis / 1000000)
    \EndIf
    \State inicio\_intervalo $\gets$ $\lfloor$evento.pub\_millis / paso$\rfloor$ $\times$ paso
    \State evento.inicio\_intervalo $\gets$ ConvertirAFecha(inicio\_intervalo)
\EndFor

\State ConvertirTipo(datos.grupo, ENTERO)
\State ConvertirTipo(datos.tipo, TEXTO)

\State datos\_filtrados $\gets$ EliminarDuplicados(datos, [inicio\_intervalo, grupo, tipo])
\State EliminarColumna(datos\_filtrados, inicio\_intervalo)
\State ConvertirAFecha(datos\_filtrados.pub\_millis)
\State ReiniciarÍndices(datos\_filtrados)

\Return datos\_filtrados
>>>>>>> 52e04f0 (feat: update models comparation)
\end{algorithmic}
\end{algorithm}


\subsection{Eventos en el espacio y tiempo}

<<<<<<< HEAD
En la \autoref{fig:dist_events} se observa la distribución espacial de los distintos tipos de eventos en la ciudad. Desde una perspectiva general se aprecian concentraciones en determinadas zonas de la ciudad.
=======
En la \cref{fig:dist_events} se observa la distribución espacial de los distintos tipos de eventos en la ciudad. Se aprecia una mayor concentración en las principales avenidas, especialmente en el caso de los accidentes, los cuales tienden a ocurrir con mayor frecuencia en los cruces de estas arterias.
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \subfloat[Accidentes]{
        \includegraphics[width=0.33\textwidth]{images/dist_accidents.png}
    }
    \subfloat[Congestión]{
        \includegraphics[width=0.33\textwidth]{images/dist_jams.png}
    }
    \subfloat[Peligros]{
        \includegraphics[width=0.33\textwidth]{images/dist_hazards.png}
    }
    \caption{Distribución espacial de eventos en Antofagasta}
    \label{fig:dist_events}
\end{figure}

<<<<<<< HEAD
Para el análisis temporal, los eventos se agruparon por hora del día, calculando el promedio de eventos ocurridos por jornada. En la \autoref{fig:time_events} se evidencia una concentración de accidentes y congestiones durante las horas punta. En contraste, los eventos de tipo \texttt{peligro} presentan una distribución más uniforme a lo largo del día.
=======
Para el análisis temporal, los eventos se agruparon por hora del día, calculando el promedio de eventos ocurridos por jornada. En la \cref{fig:time_events} se evidencia una concentración de accidentes y congestiones durante las horas punta. En contraste, los eventos de tipo \texttt{peligro} presentan una distribución más uniforme a lo largo del día.
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \subfloat[Accidentes]{
        \includegraphics[width=0.6\textwidth]{images/ACCIDENT_per_hour.png}
    }
    \newline
    \subfloat[Congestión]{
        \includegraphics[width=0.6\textwidth]{images/JAM_per_hour.png}
    }
    \newline
    \subfloat[Peligros]{
        \includegraphics[width=0.6\textwidth]{images/HAZARD_per_hour.png}
    }
    \newline
    \caption{Eventos según hora del día en Antofagasta}
    \label{fig:time_events}
\end{figure}

<<<<<<< HEAD
\subsection{Prueba de correlación lineal de Pearson}

Para evaluar la existencia de una relación estadísticamente significativa entre el número de reportes de congestión y la cantidad de accidentes por hora, se aplicó una prueba de correlación de Pearson.

La hipótesis nula ($H_0$) establece que no existe correlación lineal entre ambas variables, mientras que la hipótesis alternativa ($H_1$) plantea que sí existe tal correlación:

\begin{align}
H_0&:\ \rho = 0 \\
H_1&:\ \rho \ne 0
\end{align}

donde $\rho$ representa el coeficiente de correlación poblacional.
El estadístico de prueba se calcula mediante:

\begin{equation}
t = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}
\end{equation}

donde $r$ es el coeficiente de correlación muestral y $n$ el número de observaciones.
El valor resultante se contrasta con la distribución t de Student con $n - 2$ grados de libertad.

Los resultados obtenidos indican una correlación positiva y significativa entre ambas variables:
\begin{itemize}
    \item Días de semana: \( r = 0.96 \) (IC 95\%: [0.91, 0.98]), \(t = 16.02,\ p < 0.0001\)
    \item Fines de semana: \( r = 0.87 \) (IC 95\%: [0.71, 0.94]), \(t = 8.14,\ p < 0.0001\)
\end{itemize}

Por tanto, se rechaza la hipótesis nula con un nivel de confianza del 99\%, concluyendo que existe una relación lineal estadísticamente significativa entre el número de reportes de congestión y la cantidad de accidentes registrados por hora.

\vspace{0.5cm}

\subsection{Modelo de regresión lineal}

Con el propósito de describir y cuantificar la relación observada, se ajustó un modelo de regresión lineal simple de la forma:

\begin{equation}
y = \beta_0 + \beta_1 x + \varepsilon
\end{equation}

donde:
\begin{itemize}
    \item \(y\): número promedio de accidentes registrados por hora,
    \item \(x\): número promedio de reportes de congestión por hora,
    \item \(\beta_0\): intercepto del modelo, que representa el valor esperado de \(y\) cuando \(x = 0\),
    \item \(\beta_1\): pendiente o coeficiente de regresión, que indica el cambio promedio en \(y\) ante una unidad de incremento en \(x\),
    \item \(\varepsilon\): término de error aleatorio, asumido con media cero y varianza constante.
\end{itemize}

Los resultados del modelo lineal se presentan en la \autoref{fig:corr_lineal}.
Se observa una fuerte relación positiva durante los días de semana y una relación moderada durante los fines de semana.
=======
\subsection{Modelo de correlación lineal}

Los resultados de la regresión lineal entre accidentes y reportes de congestión, presentados en la \cref{fig:corr_lineal}, indican una fuerte correlación positiva durante los días de semana ($R^2 = 0.90$), lo que no implica necesariamente una causalidad. En cambio, durante los fines de semana la relación es más débil ($R^2 = 0.67$), aunque aún presente.

% Prueba hipótesis relación accidents-jams
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \subfloat[Día de semana]{
        \includegraphics[width=0.9\textwidth]{images/corr_lin_ds.png}
    }
    \newline
    \subfloat[Fin de semana]{
        \includegraphics[width=0.9\textwidth]{images/corr_lin_fs.png}
    }
    \newline
<<<<<<< HEAD
    \caption{Modelos de regresión lineal entre accidentes y reportes de congestión.}
    \label{fig:corr_lineal}
\end{figure}

El análisis estadístico del modelo lineal entregó los siguientes resultados:

\begin{itemize}
    \item \textbf{Días de semana:}
    \begin{itemize}
        \item Modelo ajustado: \( y = 0.1382\,x + 0.1072 \)
        \item Coeficiente de determinación: \( R^2 = 0.92 \)
        \item Significancia global: \( F(1, 22) = 256.70,\ p < 0.0001 \)
        \item Parámetros significativos: \( \beta_1 \) (\(t = 16.02,\ p < 0.0001\)); \( \beta_0 \) (\(t = 4.08,\ p < 0.001\))
    \end{itemize}

    \item \textbf{Fines de semana:}
    \begin{itemize}
        \item Modelo ajustado: \( y = 0.1205\,x + 0.0719 \)
        \item Coeficiente de determinación: \( R^2 = 0.75 \)
        \item Significancia global: \( F(1, 22) = 66.23,\ p < 0.0001 \)
        \item Parámetros significativos: \( \beta_1 \) (\(t = 8.14,\ p < 0.0001\)); \( \beta_0 \) (\(t = 9.24,\ p < 0.0001\))
    \end{itemize}
\end{itemize}

Los altos valores de \(R^2\) y la significancia de los coeficientes confirman que el número de reportes de congestión explica una proporción considerable de la variabilidad observada en los accidentes de tránsito por hora.
Sin embargo, la existencia de correlación o ajuste lineal no implica causalidad; los resultados deben contrastarse con otras fuentes de información y variables contextuales para una interpretación integral.

\subsubsection{Análisis de residuos y validación del modelo}

Los gráficos Q-Q (\autoref{fig:qq_lin}) muestran una aproximación razonable a la distribución normal en ambos períodos. Para los días de semana, los puntos siguen la línea de referencia, con algunas desviaciones en los extremos, particularmente en los valores más altos. En el caso de fin de semana, la aproximación a la línea teórica es más consistente, con una distribución más cercana a la normal.

Los gráficos de residuos (\autoref{fig:resid_lin}) revelan patrones distintos para cada período. En días de semana, aunque hay cierta dispersión alrededor de cero, se observa variabilidad considerable con algunos residuos positivos destacados (especialmente entre valores ajustados de 0.5 y 0.6) y residuos negativos pronunciados alrededor de 0.8. Para fines de semana, los residuos muestran menor magnitud global, pero un patrón menos aleatorio, sugiriendo posible estructura no capturada por el modelo lineal.

En resumen, el análisis confirma una relación lineal significativa entre congestión y accidentes, considerablemente más fuerte durante días laborables ($R^2 = 0.95$) que en fines de semana ($R^2 = 0.78$). Los patrones observados en los residuos, particularmente para fines de semana, sugieren la pertinencia de explorar modelos no lineales que podrían capturar mejor las relaciones subyacentes en los datos.
=======
    \caption{Correlación lineal entre accidentes y congestión}
    \label{fig:corr_lineal}
\end{figure}

\subsection{Prueba de hipótesis de correlación lineal}

Para evaluar la existencia de una relación estadísticamente significativa entre el número de reportes de congestión y la cantidad de accidentes por hora, se aplicó una prueba de correlación de Pearson.

La hipótesis nula ($H_0$) establece que no existe correlación lineal entre ambas variables, mientras que la hipótesis alternativa ($H_1$) plantea que sí existe tal correlación:

\begin{align}
H_0&:\ \rho = 0 \\
H_1&:\ \rho \ne 0
\end{align}

Donde $\rho$ representa el coeficiente de correlación poblacional. El estadístico de prueba se calcula mediante:

\begin{equation}
t = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}
\end{equation}

donde $r$ es el coeficiente de correlación muestral y $n$ el número de observaciones. El valor resultante se contrasta con la distribución t de Student con $n - 2$ grados de libertad.

El análisis estadístico del modelo lineal entregó los siguientes resultados:

\begin{itemize}
    \item Para días de semana:
    \begin{itemize}
        \item El modelo ajustado fue $y = 0.1673x + 0.0996$
        \item Se obtuvo un $R^2 = 0.90$
        \item La significancia global fue $F(1, 22) = 203.33$, $p < 0.0001$
        \item Los parámetros fueron estadísticamente significativos con $p < 0.001$
        \item El estadístico $t$ para la prueba de correlación fue $t = 14.26$, $p < 0.0001$
    \end{itemize}

    \item Para fines de semana:
    \begin{itemize}
        \item El modelo ajustado fue $y = 0.1365x + 0.0748$
        \item Se obtuvo un $R^2 = 0.67$
        \item El coeficiente de correlación de Pearson fue $r = 0.82$ con IC 95\%: [0.62, 0.92]
        \item La significancia global fue $F(1, 22) = 44.42$, $p < 0.0001$
        \item Los parámetros fueron estadísticamente significativos con $p < 0.0001$
        \item El estadístico $t$ para la prueba de correlación fue $t = 6.66$, $p < 0.0001$
    \end{itemize}
\end{itemize}

Estos resultados permiten rechazar la hipótesis nula con un nivel de confianza del 99\% para los días de semana, confirmando la existencia de una relación lineal estadísticamente significativa entre el número de reportes de congestión y la cantidad de accidentes registrados por hora.

Los gráficos Q-Q (\cref{fig:qq_lin}) muestran una aproximación razonable a la distribución normal en ambos períodos. Para los días de semana, los puntos siguen la línea de referencia, con algunas desviaciones en los extremos, particularmente en los valores más altos. En el caso de fin de semana, la aproximación a la línea teórica es más consistente, con una distribución más cercana a la normal.

Los gráficos de residuos (\cref{fig:resid_lin}) revelan patrones distintos para cada período. En días de semana, aunque hay cierta dispersión alrededor de cero, se observa variabilidad considerable con algunos residuos positivos destacados (especialmente entre valores ajustados de 0.5 y 0.6) y residuos negativos pronunciados alrededor de 0.8. Para fines de semana, los residuos muestran menor magnitud global, pero un patrón menos aleatorio, sugiriendo posible estructura no capturada por el modelo lineal.

En resumen, el análisis confirma una relación lineal significativa entre congestión y accidentes, considerablemente más fuerte durante días laborables ($R^2 = 0.90$) que en fines de semana ($R^2 = 0.67$). Los patrones observados en los residuos, particularmente para fines de semana, sugieren la pertinencia de explorar modelos no lineales que podrían capturar mejor las relaciones subyacentes en los datos.
>>>>>>> 52e04f0 (feat: update models comparation)


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/residuos_lin.png}
\caption{Residuos del modelo lineal para días de semana y fines de semana}
\label{fig:resid_lin}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/qq_lin.png}
\caption{Gráficos Q-Q de los residuos del modelo lineal}
\label{fig:qq_lin}
\end{figure}


<<<<<<< HEAD
A partir de la aplicación de la prueba de normalidad \textbf{D'Agostino–Pearson}, se obtuvo un valor de $p = 0.78$ para los días de semana y $p = 0.51$ para los fines de semana. En ambos casos, los valores de $p$ resultan superiores al nivel de significancia ($\alpha = 0.05$), por lo que no se rechaza la hipótesis nula de normalidad. Esto sugiere que los residuos del modelo se distribuyen aproximadamente de forma normal, indicando que las desviaciones entre los valores observados y los estimados son esencialmente aleatorias. En consecuencia, puede inferirse que la relación modelada no se encuentra significativamente afectada por variables omitidas ni por errores sistemáticos.


\subsubsection{Modelo de correlación exponencial inversa}

El análisis de residuos del modelo lineal sugirió la posible existencia de una estructura no capturada completamente por una relación estrictamente lineal,
particularmente durante los fines de semana.
Por ello, se exploró un modelo no lineal de tipo exponencial inverso, adecuado para representar relaciones donde el efecto marginal de la congestión sobre la ocurrencia de accidentes disminuye progresivamente a medida que aumenta la densidad vehicular.
=======
\subsection{Modelo de correlación exponencial inversa}

Dado que la relación entre congestión y accidentes no necesariamente es lineal en todos los contextos —especialmente bajo alta densidad vehicular— se propuso un modelo exponencial inverso.
>>>>>>> 52e04f0 (feat: update models comparation)

El modelo propuesto se ajusta a la siguiente forma funcional:

\begin{equation}
A(x) = \frac{\alpha}{x^{\beta}} + \gamma
\end{equation}

donde:
\begin{itemize}
    \item $A(x)$ es el número estimado de accidentes por hora,
    \item $x$ representa el número de reportes de congestión por hora,
    \item $\alpha$, $\beta$ y $\gamma$ son parámetros ajustados mediante regresión no lineal.
\end{itemize}

<<<<<<< HEAD
donde:
\begin{itemize}
    \item \(A(x)\): número estimado de accidentes por hora,
    \item \(x\): número de reportes de congestión por hora,
    \item \(\alpha\): parámetro de escala que controla la magnitud del efecto,
    \item \(\beta\): exponente que determina la pendiente de decrecimiento,
    \item \(\gamma\): término de desplazamiento vertical del modelo,
    \item \(\varepsilon\): término de error aleatorio.
\end{itemize}

Este modelo fue aplicado tanto a los días de semana como a los fines de semana. En ambos casos, se observó un ajuste visual considerable, respaldando la hipótesis de una relación decreciente no lineal entre ambas variables bajo ciertas condiciones de tráfico.

Los resultados del ajuste se presentan en la \autoref{fig:corr_exp}, mostrando un ajuste significativo del modelo a los datos.
=======
Este modelo fue aplicado tanto a los días de semana como a los fines de semana. En ambos casos, se observó un ajuste visual considerable, respaldando la hipótesis de una relación decreciente no lineal entre ambas variables bajo ciertas condiciones de tráfico.

Los resultados del ajuste se presentan en la \cref{fig:corr_exp}, mostrando un ajuste significativo del modelo a los datos.
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \subfloat[Día de semana]{
        \includegraphics[width=0.9\textwidth]{images/corr_exp_ds.png}
    }
    \newline
    \subfloat[Fin de semana]{
        \includegraphics[width=0.9\textwidth]{images/corr_exp_fs.png}
    }
    \newline
    \caption{Correlación exponencial inversa entre accidentes y congestión}
    \label{fig:corr_exp}
\end{figure}

El análisis estadístico del modelo exponencial inverso entregó resultados significativos tanto para días de semana como para fin de semana:

\begin{itemize}
    \item Para días de semana:
    \begin{itemize}
<<<<<<< HEAD
        \item Modelo ajustado: $y = \dfrac{0.2821}{x^{-0.6468}}$ \; (equivalente a $y = 0.2821\,x^{0.6468}$)
        \item $R^2 = 0.9520$ \; ($R^2_{\text{aj}} = 0.9499$)
        \item Significancia global: $F(2, 22) = 218.32$, $p < 0.0001$
        \item Parámetros:
              $a=0.2821$ \; ($t = 13.80$, $p < 0.0001$, IC 95\%: $[0.2397,\;0.3245]$), \;
              $b=-0.6468$ \; ($t = -13.50$, $p < 0.0001$, IC 95\%: $[-0.7461,\;-0.5475]$)
=======
        \item El modelo ajustado fue $y = 0.3119/x^{-0.6319}$
        \item Se obtuvo un $R^2 = 0.94$
        \item La significancia global fue $F(2, 22) = 158.81$, $p < 0.0001$
        \item Los parámetros fueron estadísticamente significativos con $p < 0.0001$
>>>>>>> 52e04f0 (feat: update models comparation)
    \end{itemize}

    \item Para fines de semana:
    \begin{itemize}
<<<<<<< HEAD
        \item Modelo ajustado: $y = \dfrac{0.1781}{x^{-0.3008}}$ \; (equivalente a $y = 0.1781\,x^{0.3008}$)
        \item $R^2 = 0.7838$ \; ($R^2_{\text{aj}} = 0.7740$)
        \item Significancia global: $F(2, 22) = 39.89$, $p < 0.0001$
        \item Parámetros:
              $a=0.1781$ \; ($t = 20.55$, $p < 0.0001$, IC 95\%: $[0.1602,\;0.1961]$), \;
              $b=-0.3008$ \; ($t = -7.48$, $p < 0.0001$, IC 95\%: $[-0.3841,\;-0.2174]$)
=======
        \item El modelo ajustado fue $y = 0.1826/x^{-0.2776}$
        \item Se obtuvo un $R^2 = 0.71$
        \item La significancia global fue $F(2, 22) = 27.19$, $p < 0.001$
        \item Los parámetros fueron estadísticamente significativos con $p < 0.001$
>>>>>>> 52e04f0 (feat: update models comparation)
    \end{itemize}
\end{itemize}

Este análisis revela diferencias importantes en la dinámica de la relación congestión-accidentes entre períodos. Durante los días de semana, el exponente negativo de mayor magnitud ($-0.6319$) indica una relación más sensible entre las variables, mientras que en fines de semana el exponente menor ($-0.2776$) sugiere una relación menos pronunciada.

<<<<<<< HEAD
Los gráficos de residuos (\autoref{fig:resid_exp}) muestran una dispersión relativamente aleatoria en torno a cero, sin evidencias claras de heterocedasticidad ni patrones sistemáticos, lo que respalda la validez del ajuste no lineal propuesto. En particular, para los días de semana, los residuos muestran una distribución uniforme con algunos valores extremos (entre -0.1 y 0.2), mientras que para los fines de semana, la dispersión es menor y más equilibrada (aproximadamente entre -0.05 y 0.05).

Complementariamente, los gráficos Q-Q (\autoref{fig:qq_exp}) revelan una aproximación a la distribución normal de los residuos en ambos casos, aunque con algunas desviaciones. En días de semana, se observan algunas desviaciones en los valores extremos, particularmente un punto notable en el extremo positivo. Para los fines de semana, la alineación con la línea teórica es excepcional, indicando una distribución muy cercana a la normal.
=======
Los gráficos de residuos (\cref{fig:resid_exp}) muestran una dispersión relativamente aleatoria en torno a cero, sin evidencias claras de heterocedasticidad ni patrones sistemáticos, lo que respalda la validez del ajuste no lineal propuesto. En particular, para los días de semana, los residuos muestran una distribución uniforme con algunos valores extremos (entre -0.1 y 0.2), mientras que para los fines de semana, la dispersión es menor y más equilibrada (aproximadamente entre -0.05 y 0.05).

Complementariamente, los gráficos Q-Q (\cref{fig:qq_exp}) revelan una aproximación a la distribución normal de los residuos en ambos casos, aunque con algunas desviaciones. En días de semana, se observan algunas desviaciones en los valores extremos, particularmente un punto notable en el extremo positivo. Para los fines de semana, la alineación con la línea teórica es excepcional, indicando una distribución muy cercana a la normal.
>>>>>>> 52e04f0 (feat: update models comparation)

En conjunto, estos resultados indican que el modelo exponencial inverso no solo es estadísticamente significativo, sino que además presenta un mejor ajuste que el modelo lineal, especialmente para días de semana donde alcanza un $R^2 = 0.94$ comparado con el $R^2 = 0.90$ del modelo lineal. La normalidad de los residuos y su distribución aleatoria fortalecen la hipótesis de que existe una relación no lineal entre la congestión y la probabilidad de accidentes en contextos urbanos, que puede ser modelada adecuadamente mediante una función exponencial inversa.


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/residuos_exp.png}
<<<<<<< HEAD
\caption{Residuos del modelo exponencial inverso para días de semana y fines de semana}
=======
\caption{Residuos del modelo de exponencial inverso para días de semana y fines de semana}
>>>>>>> 52e04f0 (feat: update models comparation)
\label{fig:resid_exp}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/qq_exp.png}
\caption{Gráficos Q-Q de los residuos del modelo exponencial inverso}
\label{fig:qq_exp}
\end{figure}

<<<<<<< HEAD
En el caso del modelo exponencial, se aplicó igualmente la prueba de normalidad \textbf{D'Agostino–Pearson} sobre los residuos obtenidos. Para los días de semana se obtuvo un valor $p = 0.2579$, mientras que para los fines de semana el valor fue $p = 0.6500$. Dado que en ambos casos se cumple que $p > \alpha = 0.05$, no se rechaza la hipótesis nula de normalidad. Esto indica que los residuos están distribuidos aproximadamente de manera normal, por lo que las variaciones no explicadas por el modelo pueden considerarse aleatorias y no evidencian la presencia de patrones sistemáticos o sesgos en la estimación.

En conjunto, estos resultados indican que el modelo exponencial inverso presenta un mejor ajuste que el modelo lineal, especialmente durante los días de semana.
Sin embargo, dado que ambos modelos se basan en supuestos paramétricos —particularmente la normalidad de los residuos y la homocedasticidad—, se complementó el análisis mediante un enfoque no paramétrico para verificar la robustez de la relación observada.

\subsection{Análisis de correlación no paramétrica de Spearman}

Si bien los modelos lineal y exponencial inverso mostraron ajustes estadísticamente significativos y residuos con distribución aproximadamente normal según la prueba de D’Agostino–Pearson,
resulta pertinente incorporar un enfoque no paramétrico que permita validar la robustez de los resultados sin depender de los supuestos de normalidad y linealidad.

En este contexto, se aplicó el \textbf{coeficiente de correlación de Spearman} ($\rho_s$), el cual mide la fuerza y dirección de la relación monótona entre dos variables a partir del orden de sus valores. Este enfoque es particularmente útil cuando la relación puede ser creciente o decreciente, pero no estrictamente lineal, como suele ocurrir en fenómenos de tráfico urbano donde el incremento de la congestión no siempre implica un aumento proporcional de accidentes.

El coeficiente de Spearman se define como la correlación de Pearson aplicada sobre los rangos de las observaciones, según la expresión:

\begin{equation}
\rho_s = 1 - \frac{6 \sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}
\end{equation}

donde $d_i$ representa la diferencia entre los rangos de las variables $x_i$ y $y_i$, y $n$ corresponde al número total de observaciones. Los valores de $\rho_s$ oscilan entre $-1$ y $1$, indicando respectivamente una correlación monótona negativa perfecta, nula o positiva perfecta.

Los resultados obtenidos para las correlaciones entre reportes de congestión y accidentes fueron los siguientes:

\begin{itemize}
    \item Días de semana: $\rho_s = 0.95$, $p = 1.40 \times 10^{-12}$
    \item Fin de semana: $\rho_s = 0.85$, $p = 1.04 \times 10^{-7}$
\end{itemize}

En ambos casos, los valores $p$ son significativamente inferiores al nivel de significancia adoptado ($\alpha = 0.01$), por lo que se rechaza la hipótesis nula de ausencia de correlación. Estos resultados confirman una relación monótona fuerte y positiva entre el número de reportes de congestión y la ocurrencia de accidentes, coherente con los hallazgos obtenidos en los modelos lineales y exponenciales.

La inclusión del método de Spearman aporta una \textbf{validación complementaria} desde una perspectiva no paramétrica, fortaleciendo la evidencia de asociación entre las variables analizadas. Dado que el análisis de residuos mostró normalidad y ausencia de patrones sistemáticos, la aplicación de ambos enfoques —paramétrico (Pearson y modelos lineales/exponenciales) y no paramétrico (Spearman)— permite asegurar una evaluación estadística rigurosa y robusta de la relación entre congestión y accidentes.

En síntesis, el uso combinado de modelos de regresión con análisis de residuos y correlaciones no paramétricas refuerza la consistencia de los resultados y la solidez de las conclusiones. Esta estrategia metodológica integra los beneficios de la inferencia clásica con la flexibilidad de los métodos robustos, asegurando que las conclusiones no dependan de supuestos restrictivos de distribución o forma funcional.


\subsection{Frecuencia por zona} \label{ssec:freq_zone}

Para representar conjuntos estáticos de puntos espaciales, se utiliza la estructura Kd-tree que computa distancias relativas mediante spiral codes y las almacena usando Directly Addressable Codes (DACs), optimizando el uso de memoria sin sacrificar eficiencia en consultas espaciales \parencite{gutierrez2023ckdtree}. Cada zona es llamada segmento.

En la \autoref{fig:quad_acc} se puede ver cómo se distribuyen los segmentos numéricamente, comenzando desde el segmento 1 en la esquina inferior izquierda. La opacidad del color rojo está relacionada con la cantidad de accidentes por segmento.
=======
% Cálculo de la frecuencia por zona
% Filtro por zona
%    Algóritmo cKDTree

\subsection{Frecuencia por zona} \label{ssec:freq_zone}

Para representar conjuntos estáticos de puntos espaciales, se utiliza la estructura Kd-tree que computa distancias relativas mediante spiral codes y las almacena usando Directly Addressable Codes (DACs), optimizando el uso de memoria sin sacrificar eficiencia en consultas espaciales \citep{gutierrez2023ckdtree}. Cada zona es llamada cuadrante o segmento.

En la \cref{fig:quad_acc} se puede ver cómo se distribuyen los segmentos numéricamente, comenzando desde el segmento 1 en la esquina inferior izquierda. La opacidad del color rojo está relacionada con la cantidad de accidentes por segmento.
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.38\textwidth]{images/quad_acc.png}
<<<<<<< HEAD
    \caption{Identificación de segmentos en mapa de Antofagasta}
    \label{fig:quad_acc}
\end{figure}

Considerando la media de accidentes diarios, se agrupa por segmento, se define que las zonas mayores a 0.7 accidentes por día son considerados críticos, se construye un mapa según la criticidad (\autoref{fig:quad_mean_day}), al tomar la suma de los accidentes, se obtiene la \autoref{fig:quad_acc_total} que muestra la cantidad de accidentes reportados en el segmento en el periodo de estudio, que a su vez se clasifican como críticos si poseen más de 90 accidentes.
=======
    \caption{Identificación de cuadrantes en mapa de Antofagasta}
    \label{fig:quad_acc}
\end{figure}

Considerando la media de accidentes diarios, se agrupa por segmento, se define que las zonas mayores a 0.5 accidentes por día son considerados críticos, se construye un mapa según la criticidad (\cref{fig:quad_mean_day}), al tomar la suma de los accidentes, se obtiene la \cref{fig:quad_acc_total} que muestra la cantidad de accidentes reportados en el segmento en el periodo de estudio, que a su vez se clasifican como críticos si poseen más de 90 accidentes.

>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \subfloat[Media de accidentes por día]{
        \includegraphics[width=0.4\textwidth]{images/quad_mean_day_color.png}
        \label{fig:quad_mean_day}
    }
    \subfloat[Total accidentes]{
        \includegraphics[width=0.4\textwidth]{images/quad_acc_color.png}
        \label{fig:quad_acc_total}
    }
    \caption{Accidentes por segmento}
\end{figure}


<<<<<<< HEAD
\subsection{Zonas críticas}\label{ssec:critical_zones}

A partir de la segmentación espacial descrita en la \autoref{ssec:freq_zone}, se identificaron \textit{zonas críticas} como aquellos segmentos con mayor concentración de accidentes en el período de estudio. Para ello, se computó la frecuencia total por segmento y se ordenó de forma descendente, seleccionando los cuatro con mayor conteo. La visualización de detalle para cada uno se presenta en las \autoref{fig:critical_1}--\autoref{fig:critical_4}, donde se centra el recorte en el punto medio del segmento\footnote{Notar que el centroide está alineado con el centro del segmento, no necesariamente del punto crítico.} y se resalta el entorno mediante un \textit{buffer} métrico fijo alrededor del centroide.\footnote{Los centroides se reportan en EPSG:3857 (Web Mercator).}

La \autoref{tab:critical_segments} resume la frecuencia y las coordenadas de los cuatro segmentos más incidentes. Se observa una alta concentración en pocos segmentos (221, 187, 159 y 153 accidentes, respectivamente), consistente con el mapa de criticidad de la \autoref{fig:quad_acc_total}. Este patrón evidencia \textit{puntos calientes} persistentes, útiles para priorizar medidas de gestión (señalización, fiscalización focalizada o rediseños micro-viales).

\begin{table}[H]
    \centering
    \caption{Zonas críticas: frecuencia total y centroide del segmento (EPSG:3857).}
    \label{tab:critical_segments}
    \begin{tabular}{cccc}
        \toprule
        \textbf{Top} & \textbf{Accidentes (total)} & \textbf{Centro X (m)} & \textbf{Centro Y (m)} \\
        \midrule
        1 & 221 & $-7\,835\,048.083$ & $-2\,702\,232.018$ \\
        2 & 187 & $-7\,835\,913.901$ & $-2\,709\,531.256$ \\
        3 & 159 & $-7\,835\,048.083$ & $-2\,700\,772.171$ \\
        4 & 153 & $-7\,835\,048.083$ & $-2\,706\,611.561$ \\
        \bottomrule
    \end{tabular}
\end{table}

El análisis integrado de los datos geoespaciales y temporales permitió identificar patrones consistentes de concentración de accidentes en la ciudad de Antofagasta. Entre las zonas de mayor criticidad destacan los segmentos asociados al eje \textit{Pedro Aguirre Cerda}, particularmente en su intersección con \textit{Nicolás Tirado}, donde convergen flujos vehiculares provenientes del norte y del sector costero. Asimismo, la intersección \textit{Salvador Allende–Edmundo Pérez Zujovic} presenta una elevada densidad de incidentes, atribuible a la confluencia de arterias que conectan el centro con el borde industrial. Otras áreas relevantes incluyen el cruce de \textit{Caparrosa} con \textit{Pedro Aguirre Cerda} y la intersección de \textit{Tamarugos} con \textit{Edmundo Pérez Zujovic}.

Estos resultados refuerzan la evidencia observada en la \autoref{ssec:freq_zone}, permitieron aislar sectores de riesgo sostenido. La persistencia temporal y la concentración espacial en torno a ejes estructurales sugieren que la siniestralidad responde tanto a factores de diseño vial (geometría de intersección, accesos laterales) como a condiciones operativas (volumen vehicular, flujos convergentes y patrones de movilidad urbana). De este modo, la metodología aplicada aporta una base empírica para orientar estrategias de intervención focalizada y planificación urbana sustentada en evidencia.

\begin{figure}[H]
    \centering
    \subfloat[Nicolás Tirado / Pedro Aguirre Cerda]{
        \includegraphics[width=0.49\textwidth]{images/critical_1.png}
        \label{fig:critical_1}
    }
    \subfloat[Salvador Allende / Edmundo Pérez Zujovic]{
        \includegraphics[width=0.49\textwidth]{images/critical_2.png}
        \label{fig:critical_2}
    }
    \caption{Intersecciones críticas (detalle con \textit{buffer} alrededor del centro del segmento).}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[Caparrosa / Pedro Aguirre Cerda]{
        \includegraphics[width=0.49\textwidth]{images/critical_3.png}
        \label{fig:critical_3}
    }
    \subfloat[Tamarugos / Edmundo Pérez Zujovic]{
        \includegraphics[width=0.49\textwidth]{images/critical_4.png}
        \label{fig:critical_4}
    }
    \caption{Intersecciones críticas (continuación).}
\end{figure}

=======
% Ruta más segura

% Machine learning (XGBoost)
>>>>>>> 52e04f0 (feat: update models comparation)

\subsection{Selección del modelo de Machine Learning}

%
% # RandomForest
% rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)
% rf_grid = {
%     "n_estimators": [200, 350, 500],
%     "max_depth": [None, 10, 20],
%     "min_samples_leaf": [1, 3],
%     "class_weight": [None, "balanced"],
% }
%
<<<<<<< HEAD
% [RandomForest] best CV ROC-AUC: 0.8414
% [RandomForest] best params: {'class_weight': None, 'max_depth': 20, 'min_samples_leaf': 3, 'n_estima
% tors': 500}
=======
% [RandomForest] best params: {'class_weight': None, 'max_depth': None, 'min_samples_leaf': 1, '
% n_estimators': 500}
>>>>>>> 52e04f0 (feat: update models comparation)
%
% # LogisticRegression (with scaling in a Pipeline)
% lr_pipeline = Pipeline([
%     ("scaler", StandardScaler(with_mean=False)),  # with_mean=False keeps sparse safety if present
%     ("clf", LogisticRegression(max_iter=2000, random_state=RANDOM_STATE)),
% ])
<<<<<<< HEAD
%
% [LogisticRegression] best CV ROC-AUC: 0.7566
% [LogisticRegression] best params: {'clf__C': 0.1, 'clf__class_weight': 'balanced', 'clf__penalty': '
% l2', 'clf__solver': 'lbfgs'}
=======
% lr_grid = {
%     "clf__penalty": ["l2"],
%     "clf__solver": ["lbfgs", "liblinear"],
%     "clf__C": [0.1, 1.0, 10.0],
%     "clf__class_weight": [None, "balanced"],
% }
%
%
% [LogisticRegression] best params: {'clf__C': 10.0, 'clf__class_weight': 'balanced', 'clf__pena
% lty': 'l2', 'clf__solver': 'lbfgs'}
>>>>>>> 52e04f0 (feat: update models comparation)
%
% # XGBClassifier
% xgb = XGBClassifier(
%     random_state=RANDOM_STATE,
%     n_estimators=400,
%     objective="binary:logistic",
%     tree_method="hist",
%     eval_metric="auc",
%     n_jobs=-1,
% )
% xgb_grid = {
%     "max_depth": [3, 6, 10, 20],
%     "n_estimators": [80, 100],
%     "learning_rate": [0.1, 0.03],
%     "gamma": [0.8, 1.0],
%     "subsample": [0.8, 1.0],
%     "colsample_bytree": [0.8, 1.0],
%     "min_child_weight": [1, 3],
% }
%
<<<<<<< HEAD
% [XGBoost] best CV ROC-AUC: 0.8480
% [XGBoost] best params: {'colsample_bytree': 0.8, 'gamma': 1.0, 'learning_rate': 0.1, 'max_depth': 10
% , 'min_child_weight': 1, 'n_estimators': 80, 'subsample': 0.8}
%
%
%                 model  accuracy  precision    recall        f1   roc_auc    pr_auc
% 0             XGBoost  0.771112   0.738636  0.839156  0.785694  0.843327  0.810811
% 1        RandomForest  0.766935   0.734552  0.835966  0.781985  0.839358  0.807292
% 2  LogisticRegression  0.717269   0.672954  0.845383  0.749377  0.746327  0.659040
%
% RandomForest — Confusion Matrix
% [[4595 1989]
%  [1080 5504]]
% TN=4595, FP=1989, FN=1080, TP=5504
%
% LogisticRegression — Confusion Matrix
% [[3879 2705]
%  [1018 5566]]
% TN=3879, FP=2705, FN=1018, TP=5566
%
% XGBoost — Confusion Matrix
% [[4629 1955]
%  [1059 5525]]
% TN=4629, FP=1955, FN=1059, TP=5525
%

Se evaluaron tres modelos de clasificación para seleccionar el de mejor rendimiento.
La sintonía de hiperparámetros se realizó mediante una búsqueda exhaustiva (\textit{GridSearchCV}) utilizando como métrica principal el área bajo la curva ROC (\texttt{ROC–AUC}).
Los modelos considerados fueron \textbf{Random Forest}, \textbf{Regresión Logística} y \textbf{XGBoost}.

\paragraph{Random Forest.}

\begin{table}[H]
\centering
\caption{Mejores hiperparámetros obtenidos para el modelo \textit{Random Forest}.}
\label{tab:rf-params}
\begin{tabular}{lc}
\toprule
\textbf{Hiperparámetro} & \textbf{Valor óptimo} \\
\midrule
\texttt{n\_estimators}       & 100  \\
\texttt{max\_depth}          & 10   \\
\texttt{min\_samples\_leaf}  & 2    \\
\texttt{max\_features}       & 2    \\
\texttt{max\_leaf\_nodes}    & 8    \\
\texttt{class\_weight}       & None \\
\bottomrule
\end{tabular}
\end{table}

Con estos valores el modelo alcanzó un \textbf{ROC–AUC de 0.8394} y un \textbf{PR–AUC de 0.8073} (véase \autoref{tab:model-comparison}).

\paragraph{Regresión Logística.}

\begin{table}[H]
\centering
\caption{Mejores hiperparámetros obtenidos para el modelo de \textit{Regresión Logística}.}
\label{tab:lr-params}
\begin{tabular}{lc}
\toprule
\textbf{Hiperparámetro} & \textbf{Valor óptimo} \\
\midrule
\texttt{penalty}   & l2 \\
\texttt{solver}    & lbfgs \\
\texttt{max\_iter} & 1000 \\
\bottomrule
\end{tabular}
\end{table}

El modelo obtuvo un \textbf{ROC–AUC de 0.7463} y un \textbf{PR–AUC de 0.6590} (\autoref{tab:model-comparison}).

\paragraph{XGBoost.}

\begin{table}[H]
\centering
\caption{Mejores hiperparámetros obtenidos para el modelo \textit{XGBoost}.}
\label{tab:xgb-params}
\begin{tabular}{lc}
\toprule
\textbf{Hiperparámetro} & \textbf{Valor óptimo} \\
\midrule
\texttt{max\_depth}        & 30   \\
\texttt{n\_estimators}     & 80   \\
\texttt{learning\_rate}    & 0.1  \\
\texttt{gamma}             & 1    \\
\texttt{colsample\_bytree} & 0.7  \\
\bottomrule
\end{tabular}
\end{table}

Con esta configuración, \textit{XGBoost} alcanzó la mejor puntuación promedio, con un \textbf{ROC–AUC de 0.8433} y un \textbf{PR–AUC de 0.8108}, superando ligeramente a \textit{Random Forest} y de forma clara a la \textit{Regresión Logística} (\autoref{tab:model-comparison}).

Los gráficos de la curva ROC y de la curva Precision–Recall se presentan en las \autoref{fig:ROC} y \autoref{fig:prec-recall}, respectivamente.

En el \autoref{tab:model-comparison} se puede observar la comparación de los modelos.

\begin{table}[H]
\centering
\caption{Comparativa de modelos ajustados con \texttt{GridSearchCV} según el área bajo las curvas ROC y Precision–Recall (PR AUC).}
\label{tab:model-comparison}
\begin{tabular}{l p{6cm} c c}
\toprule
\textbf{Modelo} & \textbf{Mejores Hiperparámetros} & \textbf{ROC AUC} & \textbf{PR AUC} \\
\midrule
=======
% [XGBoost] best params: {'colsample_bytree': 0.8, 'gamma': 0.8, 'learning_rate': 0.1, 'max_dept
% h': 20, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}
%
%                 model  accuracy  precision    recall        f1   roc_auc    pr_auc
% 0             XGBoost  0.829609   0.812915  0.856283  0.834036  0.910052  0.899817
% 1        RandomForest  0.827351   0.804550  0.864786  0.833581  0.907710  0.898675
% 2  LogisticRegression  0.721181   0.686955  0.812716  0.744563  0.780227  0.714036
%

Se evaluaron tres modelos de clasificación para seleccionar el con mejor rendimiento, para ello primero se seleccionaron los mejores hiperparámetros para cada uno basado en la \texxttt{curva ROC-AUC}, realizando una búsqueda exhaustiva (\textit{GridSearchCV}) sobre los tres modelos: \textbf{Random Forest}, \textbf{Logistic Regression} y \textbf{XGBoost}.

\paragraph{Random Forest}
Se evaluaron los siguientes hiperparámetros:

\begin{itemize}
    \item \texttt{n\_estimators}: [200, 350, 500]
    \item \texttt{max\_depth}: [None, 10, 20]
    \item \texttt{min\_samples\_leaf}: [1, 3]
    \item \texttt{class\_weight}: [None, 'balanced']
\end{itemize}

Los mejores hiperparámetros obtenidos fueron:

\begin{itemize}
    \item \texttt{n\_estimators}: 500
    \item \texttt{max\_depth}: None
    \item \texttt{min\_samples\_leaf}: 1
    \item \texttt{class\_weight}: None
\end{itemize}

Con estos valores, el modelo alcanzó una puntuación máxima de \textbf{0.7957} en validación cruzada.

\paragraph{Logistic Regression}
Se evaluaron los siguientes hiperparámetros:

\begin{itemize}
    \item \texttt{clf\_\_penalty}: [l2]
    \item \texttt{clf\_\_solver}: ['lbfgs', 'liblinear']
    \item \texttt{clf\_\_C}: [0.1, 1.0, 10.0]
    \item \texttt{clf\_\_class\_weight}: [None, 'balanced']
\end{itemize}

Los mejores hiperparámetros obtenidos fueron:

\begin{itemize}
    \item \texttt{clf\_\_penalty}: l2
    \item \texttt{clf\_\_solver}: 'lbfgs'
    \item \texttt{clf\_\_C}: 10.0
    \item \texttt{clf\_\_class\_weight}: 'balanced'
\end{itemize}


\paragraph{XGBoost.}

Se evaluaron los siguientes hiperparámetros:

\begin{itemize}
    \item \texttt{colsample\_bytree}: [0.8, 1.0]
    \item \texttt{gamma}: [0.8, 1.0]
    \item \texttt{learning\_rate}: [0.1, 0.03]
    \item \texttt{max\_depth}: [3, 6, 10, 20]
    \item \texttt{min\_child\_weight}: [1, 3]
    \item \texttt{n\_estimators}: [80, 100]
    \item \texttt{subsample}: [0.8, 1.0]
\end{itemize}

Los mejores hiperparámetros obtenidos fueron:

\begin{itemize}
    \item \texttt{colsample\_bytree}: 0.8
    \item \texttt{gamma}: 0.8
    \item \texttt{learning\_rate}: 0.1
    \item \texttt{max\_depth}: 20
    \item \texttt{min\_child\_weight}: 1
    \item \texttt{n\_estimators}: 100
    \item \texttt{subsample}: 0.8
\end{itemize}

Este modelo logró una valor de \texttt{curva ROC-AUC} de \textbf{0.910052}, superando al resto de las alternativas evaluadas. Por este motivo, \textbf{XGBoost fue seleccionado como modelo final}.

En el \cref{tab:model-comparison} se puede observar la comparación de los modelos.

\begin{table}[H]
\centering
\begin{tabular}{|l|p{8cm}|c|}
\hline
\textbf{Modelo} & \textbf{Mejores Hiperparámetros} & \textbf{Mejor Score (ROC AUC)} \\
\hline
>>>>>>> 52e04f0 (feat: update models comparation)
\textbf{Random Forest} &
\begin{tabular}[c]{@{}l@{}}
\texttt{class\_weight}: None\\
\texttt{max\_depth}: 10\\
\texttt{max\_features}: 2\\
\texttt{max\_leaf\_nodes}: 8\\
\texttt{min\_samples\_leaf}: 2\\
\texttt{n\_estimators}: 100
\end{tabular}
<<<<<<< HEAD
& 0.8394 & 0.8073 \\
\midrule
=======
& 0.9077 \\
\hline
>>>>>>> 52e04f0 (feat: update models comparation)
\textbf{XGBoost} &
\begin{tabular}[c]{@{}l@{}}
\texttt{colsample\_bytree}: 0.7\\
\texttt{gamma}: 1\\
\texttt{learning\_rate}: 0.1\\
\texttt{max\_depth}: 30\\
\texttt{n\_estimators}: 80
\end{tabular}
<<<<<<< HEAD
& \textbf{0.8433} & \textbf{0.8108} \\
\midrule
=======
& \textbf{0.9101} \\
\hline
>>>>>>> 52e04f0 (feat: update models comparation)
\textbf{Regresión Logística} &
\begin{tabular}[c]{@{}l@{}}
\texttt{penalty}: l2\\
\texttt{solver}: lbfgs\\
\texttt{max\_iter}: 1000
\end{tabular}
<<<<<<< HEAD
& 0.7463 & 0.6590 \\
\bottomrule
\end{tabular}
\end{table}

Los gráficos de la curva ROC están representados en la \autoref{fig:ROC} y de la curva Precision-Recall en la \autoref{fig:prec-recall}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/ROC.png}
    \caption{Curvas ROC}
    \label{fig:ROC}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/Precision-Recall.png}
    \caption{Curvas Precision-Recall}
    \label{fig:prec-recall}
\end{figure}

\subsection{Matriz de confusión y desempeño comparativo de los modelos}

La \autoref{tab:confusion_matrix_models} presenta un resumen de las matrices de confusión obtenidas para los tres modelos de clasificación evaluados: \textit{Random Forest}, \textit{Logistic Regression} y \textit{XGBoost}.
Cada matriz permite analizar la distribución de aciertos y errores en las predicciones, considerando las métricas \textit{True Negatives} (TN), \textit{False Positives} (FP), \textit{False Negatives} (FN) y \textit{True Positives} (TP).
En términos generales, el modelo \textit{XGBoost} logra el mejor equilibrio entre sensibilidad y especificidad, con un menor número de falsos positivos y un alto número de verdaderos positivos, lo que evidencia su capacidad de generalización frente a los otros algoritmos.

\begin{table}[H]
\centering
\caption{Resumen de matrices de confusión para los modelos de clasificación evaluados.}
\label{tab:confusion_matrix_models}
\begin{tabular}{lcccc}
\toprule
\textbf{Modelo} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\
\midrule
Random Forest        & 4567 & 2017 & 1090 & 5494 \\
Logistic Regression  & 3877 & 2707 & 1019 & 5565 \\
XGBoost              & 4644 & 1940 & 1066 & 5518 \\
\bottomrule
\end{tabular}
\end{table}

A partir de los valores presentados, se observa que el modelo \textit{XGBoost} mantiene una proporción más favorable entre verdaderos positivos y falsos positivos, lo que sugiere un mejor rendimiento en la identificación de eventos reales de congestión sin un exceso de falsas alarmas.
En contraste, \textit{Logistic Regression} presenta el mayor número de falsos positivos, reflejando una menor capacidad discriminativa frente a los otros métodos.
Finalmente, \textit{Random Forest} muestra un comportamiento intermedio, con un desempeño aceptable pero ligeramente inferior al de \textit{XGBoost} en términos de precisión global.

\subsection{Generación de eventos negativos simulados} \label{ssec:class_balancing}

Dado que el conjunto original contiene únicamente eventos positivos (ocurridos), fue necesario generar instancias negativas (no ocurridos) para entrenar un modelo de clasificación binaria. Para ello, se implementó una estrategia inspirada en el enfoque de generación de eventos negativos artificiales propuesto por Goedertier et al. \parencite{goedertier2009robust}, el cual permite representar problemas secuenciales como tareas de clasificación supervisada.\footnote{Este enfoque evita el uso de sobremuestreo sintético como SMOTE, reduciendo la distorsión de la distribución original de los datos.}

A diferencia del algoritmo original, en el caso del tráfico vehicular se adoptó una relación 1:1 entre eventos positivos y negativos —esto es, se genera un evento negativo por cada evento positivo—, priorizando la eficiencia mediante operaciones vectorizadas y una estructura de datos matricial. Además, debido a que cada evento es independiente de los demás, no se aplicó la noción de causalidad ni las restricciones de paralelismo introducidas por Goedertier, propias de los procesos secuenciales.

En este contexto, la generación de eventos negativos se basa en una grilla temporal con intervalos de 5 minutos, sobre la cual se combinan todas las posibles ubicaciones (\texttt{segmento}) y tipos de evento. Se identifican aquellas combinaciones que no se encuentran en los datos originales —donde no ocurrió un evento— y se etiquetan como eventos negativos (\texttt{happen} = 0). Estas instancias se muestrean aleatoriamente hasta igualar la cantidad de eventos positivos, obteniendo así un conjunto balanceado con un 50\% de instancias positivas y 50\% negativas.

Este enfoque permite preservar la estructura temporal y geoespacial de los datos reales, evitando introducir ruido artificial y facilitando una evaluación más robusta de métricas de desempeño como \texttt{recall} y \texttt{precision}.

El procedimiento se resume en el \autoref{alg:neg_events}. El código completo se incluye en el \autoref{annex:neg_code}.

\begin{algorithm}[H]
\caption{Generación de eventos negativos artificiales}
\label{alg:neg_events}
\begin{algorithmic}[1]
\Require Conjunto de eventos positivos $D$, atributo geográfico $g$, tamaño de intervalo $t = 5$ minutos
\Ensure Conjunto balanceado $D'$ con eventos positivos y negativos

\State Convertir marcas de tiempo a milisegundos y definir intervalos temporales de tamaño $t$
\State Agrupar eventos por combinación de $(intervalo, g, tipo)$
\State Generar todas las combinaciones posibles de $(intervalo, g, tipo)$
\State Identificar combinaciones ausentes en $D$ como eventos negativos (\texttt{happen} = 0)
\State Muestrear aleatoriamente los eventos negativos hasta igualar la cantidad de positivos
\State Asignar ubicaciones aleatorias dentro del mismo grupo geográfico $g$
\State Combinar eventos positivos y negativos en un único conjunto balanceado $D'$
\State \Return $D'$
\end{algorithmic}
\end{algorithm}
=======
& 0.7802 \\
\hline
\end{tabular}
\caption{Comparativa de modelos ajustados con \texttt{GridSearchCV} según el área bajo la curva ROC (ROC AUC).}
\label{tab:model-comparison}
\end{table}

\subsection{Generación de eventos negativos simulados} \label{ssec:class_balancing}

Dado que el conjunto original contiene únicamente eventos positivos (ocurridos), fue necesario generar instancias negativas (no ocurridos) para entrenar un modelo de clasificación binaria. Para ello, se implementó una estrategia inspirada en el enfoque de generación de eventos negativos artificiales propuesto por Goedertier et al. \citep{goedertier2009robust}, que permite representar problemas secuenciales como tareas de clasificación supervisada.

En este caso, la generación se basa en una grilla temporal con intervalos de 5 minutos, sobre la cual se combinan todas las posibles ubicaciones (\texttt{segmento}) y tipos de evento. Se identifican aquellas combinaciones que no se encuentran en los datos originales —donde no ocurrió un evento— y se etiquetan como eventos negativos (\texttt{happen} = 0). Estas instancias se muestrean de forma aleatoria para igualar la cantidad de eventos positivos, logrando así un conjunto balanceado con un 50\% de eventos positivos y 50\% de negativos.

Este enfoque permite mantener la estructura temporal y geoespacial de los datos reales, evitando introducir ruido aleatorio, y facilitando la evaluación robusta de métricas como \texttt{recall} y \texttt{precision}.

>>>>>>> 52e04f0 (feat: update models comparation)

\subsection{Resultados del modelo seleccionado}

La versión final del modelo (\texttt{v6}) se entrenó utilizando los hiperparámetros obtenidos durante la etapa de validación cruzada. El modelo fue registrado y versionado mediante \textit{MLflow}, permitiendo un seguimiento detallado de sus parámetros y métricas.

<<<<<<< HEAD
El modelo utiliza codificación \textit{one-hot} y un esquema de clasificación binaria (\texttt{binary: logistic}), con una semilla aleatoria (\texttt{random\_state = 42}) para garantizar la reproducibilidad \parencite{geron2019hands}


\noindent En cuanto al rendimiento, se obtuvieron los resultados que se resumen en la \autoref{tab:model-performance}, calculados sobre una muestra de 52{,}672 eventos, con una proporción balanceada de 50\% de ocurrencias y 50\% de no ocurrencias.
Este equilibrio se logró mediante la simulación de eventos negativos sobre una grilla temporal de 5 minutos, generando combinaciones posibles de tiempo, localización y tipo de evento no presentes en los datos originales (\autoref{ssec:class_balancing}).
Este procedimiento permite evaluar el modelo en condiciones controladas y facilita la interpretación de métricas como \textit{precision}, \textit{recall} y \textit{F1-score}.

\begin{table}[H]
\centering
\caption{Métricas de rendimiento del modelo \textit{XGBoost} en conjunto de validación balanceado.}
\label{tab:model-performance}
\begin{tabular}{l c}
\toprule
\textbf{Métrica} & \textbf{Valor} \\
\midrule
Exactitud (\textit{Accuracy}) & 0.8795 \\
Recuperación (\textit{Recall}) & 0.9076 \\
Precisión (\textit{Precision}) & 0.8583 \\
Medida F1 (\textit{F1-score}) & 0.8823 \\
Área bajo la curva ROC (\textit{ROC–AUC}) & 0.8433 \\
Área bajo la curva Precision–Recall (\textit{PR–AUC}) & 0.8108 \\
\bottomrule
\end{tabular}
\end{table}

Estas métricas evidencian un modelo con alta capacidad para identificar correctamente los casos positivos, manteniendo un equilibrio adecuado entre sensibilidad y precisión.
Los valores de área bajo las curvas ROC y Precision–Recall confirman un poder discriminativo consistente en distintos umbrales de decisión.

El modelo se entrenó y evaluó utilizando particiones aleatorias del conjunto de datos (70\% entrenamiento y 30\% validación), verificándose estabilidad en las métricas a lo largo de distintas ejecuciones y sin evidencia de sobreajuste.

\subsection{Curva de aprendizaje del modelo}

La \autoref{fig:learning_curve} presenta la curva de aprendizaje del modelo \textit{XGBoost} entrenado. Se observa un desempeño alto y estable tanto en el conjunto de entrenamiento como en el de validación (valores de exactitud entre 0.80 y 0.89), con una brecha reducida y constante entre ambas curvas.

Este comportamiento indica que el modelo ha alcanzado un nivel de generalización adecuado, sin evidencias de sobreajuste ni subajuste. Además, la forma aproximadamente horizontal de las curvas sugiere que el incremento del tamaño del conjunto de entrenamiento no produciría mejoras significativas en el rendimiento, lo que refleja una saturación en la capacidad de aprendizaje con las características actuales \parencite{raschka2018, chen2015}.
=======
El modelo utiliza codificación \textit{one-hot} y un esquema de clasificación binaria (\texttt{binary: logistic}), con una semilla aleatoria (\texttt{random\_state = 42}) para garantizar la reproducibilidad \citep{geron2019hands}


\noindent En cuanto al rendimiento, se obtuvieron los siguientes resultados para una muestra de 64,708 eventos, con un 50\% de eventos ocurridos y un 50\% de no ocurridos. Esta proporción se logró mediante la simulación de eventos negativos sobre una grilla temporal de 5 minutos, generando combinaciones posibles de tiempo, localización y tipo de evento que no están presentes en los datos originales (\cref{ssec:class_balancing}). Esto permite evaluar el modelo en condiciones balanceadas, facilitando la interpretación de métricas como \texttt{precision}, \texttt{recall} y \texttt{F1-score}.

\begin{itemize}
    \item \textbf{Accuracy}: 0.8795
    \item \textbf{Recall}: 0.9076
    \item \textbf{Precision}: 0.8583
    \item \textbf{F1-score}: 0.8823
    \item \textbf{Promedio validación cruzada (10 folds)}: 0.7819
\end{itemize}

\noindent Respecto a la matriz de confusión, se registraron los siguientes valores:

\begin{itemize}
    \item Verdaderos positivos: 5,539
    \item Verdaderos negativos: 5,843
    \item Falsos positivos: 965
    \item Falsos negativos: 595
\end{itemize}

Estas métricas evidencian un modelo con gran capacidad para identificar correctamente los casos positivos, sin sacrificar precisión ni equilibrio. La validación cruzada muestra estabilidad a lo largo de los distintos \texttt{folds}, lo que refuerza la robustez del modelo final y confirma que no hubo sobreentrenamiento \citep{geron2019hands}.

\subsection{Curva de aprendizaje del modelo}

La \cref{fig:learning_curve} muestra la curva de aprendizaje del modelo XGBoost entrenado. Se observa un rendimiento alto y consistente tanto en el conjunto de entrenamiento como en el de validación (aproximadamente 0.88–0.90), con una brecha mínima entre ambas curvas. Esto indica que el modelo generaliza adecuadamente, sin presentar sobreajuste ni subajuste. Además, el comportamiento plano de ambas curvas sugiere que incrementar el tamaño del conjunto de entrenamiento no tendría un impacto significativo en el rendimiento, lo cual evidencia una saturación en la capacidad de aprendizaje del modelo con las características actuales \citep{raschka2018,chen2016}.
>>>>>>> 52e04f0 (feat: update models comparation)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/learning_curve.png}
<<<<<<< HEAD
    \caption{Curva de aprendizaje del modelo \textit{XGBoost}: desempeño en entrenamiento con validación cruzada estratificada de 20 particiones.}
    \label{fig:learning_curve}
\end{figure}

El entrenamiento del modelo está programado para realizarse cada 30 días, utilizando los nuevos datos. Los datos del dashboard se actualizan cada 5 minutos.

\subsection{Despliegue del modelo y panel de visualización}

Con el objetivo de facilitar la interpretación de los resultados y el uso práctico del modelo predictivo, se desarrolló un \textit{dashboard} interactivo que integra tanto los análisis descriptivos como las predicciones generadas por el sistema. En la \autoref{fig:dashboard_general} se presenta una vista global del panel; en particular, la sección de análisis descriptivo se ilustra en la \autoref{fig:dash_descriptivo} y la sección dedicada al modelo predictivo en la \autoref{fig:dash_predictivo}.

El \textit{dashboard} fue implementado en \textit{Python} utilizando el framework \textit{Dash}, integrando directamente los resultados del modelo \textit{XGBoost} con componentes visuales interactivos. La interfaz cuenta con filtros por calle y rango de fechas, posibilitando una exploración detallada de los eventos registrados. En la sección predictiva, el usuario puede ajustar los parámetros de entrada —como fecha, hora y tipo de evento (accidente, congestión u peligro)— para obtener proyecciones específicas del comportamiento del tráfico.

El sistema se encuentra desplegado en línea y disponible públicamente a través del sitio web \url{https://traficoantofagasta.com}. La estructura técnica y la arquitectura del sistema (cliente y \textit{backend}) se describen de forma general en el \autoref{annex:dashboard}.

\begin{figure}[H]
    \centering
    \subfloat[Sección de análisis descriptivo]{
        \includegraphics[height=7cm]{images/dash1.png}
        \label{fig:dash_descriptivo}
    }
    \hfill
    \subfloat[Sección del modelo predictivo]{
        \includegraphics[height=7cm]{images/dash2.png}
        \label{fig:dash_predictivo}
    }
    \caption{Vista general del \textit{dashboard} desarrollado para el monitoreo y predicción del tráfico vehicular.}
    \label{fig:dashboard_general}
\end{figure}

=======
    \caption{Curva de aprendizaje del modelo XGBoost: desempeño en entrenamiento y validación.}
    \label{fig:learning_curve}
\end{figure}

% Aplicación implementación (dashboard online y ML)

\subsection{Despliegue y mantenimiento del modelo}

Para entregar una interfaz que permita al usuario final interactuar con los datos, se desarrolló un tablero de control utilizando \texttt{Dash}, un marco de trabajo especializado en la presentación de datos gráficos mediante Python. La arquitectura del sistema se diseñó siguiendo el patrón cliente-servidor: el servidor implementado en Rust y el cliente en Python, como se muestra en la \cref{fig:architecture}. El servidor se encarga de las operaciones intensivas como la captura, transformación, almacenamiento y gestión del caché de los datos, mientras que el cliente maneja la interacción con el usuario, la actualización de datos desde el servidor, el entrenamiento del modelo y la generación del tablero de control.

El ciclo de vida del modelo se gestiona mediante APScheduler. Esto permite automatizar el re-entrenamiento periódico del modelo con nuevos datos, garantizando su actualización y adaptación a los cambios en los patrones de tráfico. El proceso incluye la validación automática del rendimiento del modelo, comparando las métricas clave (precisión, recall, F1-score) con los umbrales establecidos, y solo se despliega una nueva versión si representa una mejora en el F1-score.

En el servidor se utiliza Memcached como sistema de caché para optimizar el acceso a los datos frecuentemente consultados, implementando una estrategia de almacenamiento que prioriza la unicidad de los datos para minimizar el uso de memoria. En el lado del cliente, se implementa un sistema de workers múltiples que comparten una única instancia de los datos en memoria, protegida mediante un \texttt{Mutex} para garantizar la integridad en las operaciones de lectura concurrentes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dashboard.png}
    \caption{Arquitectura del sistema}
    \label{fig:architecture}
\end{figure}

El entrenamiento del modelo está programado para realizarse cada 30 días, utilizando los nuevos datos. Los datos del dashboard se actualizan cada 5 minutos.

>>>>>>> 52e04f0 (feat: update models comparation)
\section{Discusiones}

\subsection{Patrones espaciales y temporales del tráfico}

<<<<<<< HEAD
El análisis integrado de los datos geoespaciales y temporales permitió identificar patrones consistentes de concentración de accidentes en la ciudad de Antofagasta. Entre las zonas con mayor criticidad destacan los segmentos asociados al eje \textit{Pedro Aguirre Cerda}, particularmente en su intersección con \textit{Nicolás Tirado}, donde convergen flujos vehiculares provenientes del norte y del sector costero. Asimismo, la intersección \textit{Salvador Allende–Edmundo Pérez Zujovic} presenta una elevada densidad de incidentes, atribuible a la confluencia de arterias que conectan el centro con el borde industrial. Otras áreas relevantes incluyen el cruce de \textit{Caparrosa} con \textit{Pedro Aguirre Cerda} y la intersección de \textit{Tamarugos} con \textit{Edmundo Pérez Zujovic}.

El análisis temporal evidenció una correlación entre la frecuencia de accidentes y la congestión del tráfico, en línea con la literatura sobre tráfico urbano \parencite{berhanu2024}. Esto sugiere que podría existir una relación de causalidad entre ambas variables, lo cual debe ser evidenciado utilizando métodos adicionales para la investigación.
=======
El análisis integrado de datos geoespaciales y temporales permitió identificar patrones críticos de siniestralidad en Antofagasta. Destacan como focos principales los segmentos del eje Pedro Aguirre Cerda, especialmente el cruce con Nicolás Tirado (segmentos 106–109), así como la intersección Salvador Allende–Edmundo Pérez Zujovic (segmentos 84 y 103), donde confluyen flujos significativos de vehículos desde distintos sectores de la ciudad. El centro urbano (segmento 82) también presenta una elevada concentración de incidentes.

El análisis temporal evidenció una correlación entre la frecuencia de accidentes y la congestión del tráfico, en línea con la literatura sobre tráfico urbano \citep{berhanu2024}. Esto sugiere que podría existir una relación de causalidad entre ambas variables, lo cual debe ser evidenciado utilizando métodos adicionales para la investigación.
>>>>>>> 52e04f0 (feat: update models comparation)

\subsection{Desempeño y comparación de modelos predictivos}

En la modelación estadística, se observó una correlación significativa entre la congestión reportada y la ocurrencia de accidentes, especialmente en días laborales. El modelo exponencial inverso logró un ajuste superior ($R^2 = 0.96$) respecto al modelo lineal ($R^2 = 0.92$).

<<<<<<< HEAD
El modelo XGBoost logró un desempeño destacado en la predicción de accidentes, con un \textbf{F1-score de 0.88} y un \textbf{Recall de 0.91} en el conjunto de validación. Los resultados obtenidos sugieren que los algoritmos de aprendizaje automático como XGBoost pueden capturar relaciones no lineales y complejas entre las variables predictoras.

El análisis temporal sugiere una mayor frecuencia de accidentes en días hábiles y horarios punta. Se debe contrastar esta relación utilizando nuevas fuentes de información como puede ser la visión por computadora o evaluaciones en terreno \parencite{goodall2019}.

\subsection{Aporte del análisis geoespacial}

El enfoque geoespacial permitió visualizar con claridad zonas críticas y patrones de concentración que no serían evidentes mediante análisis tabulares convencionales. Las visualizaciones generadas, como la \autoref{fig:quad_acc_total} y \autoref{fig:quad_mean_day}, ofrecen una herramienta valiosa para autoridades y planificadores, facilitando la priorización de intervenciones y el diseño de políticas viales focalizadas. Sin embargo, la calidad de estos análisis depende de la completitud y precisión de los datos de referencia, lo cual puede limitar su aplicabilidad en zonas con cartografía incompleta o baja densidad de reportes.

\subsection{Implicancias prácticas y futuras aplicaciones}

El sistema desarrollado evidencia el potencial de los datos colaborativos y el aprendizaje automático para mejorar la gestión del tráfico en ciudades de tamaño medio. Su arquitectura modular y su enfoque basado en datos abiertos facilitan la replicabilidad en otras urbes chilenas. Además, la actualización periódica del modelo y la visualización en tiempo real abren la puerta a su integración con sistemas de gestión vial, aplicaciones móviles o programas de prevención de accidentes de tránsito.

Futuras aplicaciones podrían incluir la incorporación de datos meteorológicos, información de infraestructura o sistemas adicionales que permitan mejorar la confiabilidad del modelo y poder gestionar de forma preventiva el tránsito vehicular.
=======
El modelo XGBoost logró un desempeño destacado en la predicción de accidentes, con un \textbf{F1-score de 0.88} y un \textbf{Recall de 0.91} en el conjunto de validación. Si bien no se realizó una comparación directa y sistemática con todos los modelos clásicos de series temporales, los resultados obtenidos sugieren que los algoritmos de aprendizaje automático como XGBoost pueden capturar relaciones no lineales y complejas entre las variables predictoras.

El análisis temporal sugiere una mayor frecuencia de accidentes en días hábiles y horarios punta. Se debe contrastar esta relación utilizando nuevas fuentes de información como puede ser la visión por computadora o evaluaciones en terreno \citep{goodall2019}.

\subsection{Aporte del análisis geoespacial}

El enfoque geoespacial permitió visualizar con claridad zonas críticas y patrones de concentración que no serían evidentes mediante análisis tabulares convencionales. Las visualizaciones generadas, como la \cref{fig:quad_acc_total} y \cref{fig:quad_mean_day}, ofrecen una herramienta valiosa para autoridades y planificadores, facilitando la priorización de intervenciones y el diseño de políticas viales focalizadas. Sin embargo, la calidad de estos análisis depende de la completitud y precisión de los datos de referencia, lo cual puede limitar su aplicabilidad en zonas con cartografía incompleta o baja densidad de reportes.

\subsection{Implicancias prácticas y futuras aplicaciones}

El sistema desarrollado evidencia el potencial de los datos colaborativos y el aprendizaje automático para mejorar la gestión del tráfico en ciudades de tamaño medio. Su arquitectura modular y su enfoque basado en datos abiertos facilitan la replicabilidad en otras urbes chilenas. Además, la actualización periódica del modelo y la visualización en tiempo real abren la puerta a su integración con sistemas de gestión vial, aplicaciones móviles o incluso plataformas de respuesta ante emergencias y eventos masivos.

Futuras aplicaciones podrían incluir la incorporación de datos meteorológicos, información de infraestructura o variables de comportamiento humano, así como la exploración de modelos más avanzados (por ejemplo, LSTM o transformers) que capturen mejor la naturaleza secuencial y dinámica del tráfico.
>>>>>>> 52e04f0 (feat: update models comparation)

\subsection{Limitaciones del estudio}

El presente trabajo enfrentó diversas restricciones asociadas principalmente a la disponibilidad y calidad de los datos utilizados. Destaca la ausencia de fuentes independientes que permitieran contrastar y validar físicamente los reportes de incidentes, así como la falta de acceso a información relevante sobre la operación de semáforos y las características específicas de los accidentes (por ejemplo, gravedad, causa o participantes involucrados). Esta carencia de datos complementarios limita la posibilidad de validar exhaustivamente los resultados obtenidos. Adicionalmente, no se contó con evidencia en video ni registros visuales que permitieran corroborar la ocurrencia y naturaleza de los eventos reportados.

La generalización de los resultados debe abordarse con cautela, considerando las particularidades urbanas y culturales de cada ciudad, así como las limitaciones del enfoque aplicado. Finalmente, la adopción de modelos complejos puede suponer un desafío adicional en cuanto a la transparencia y comprensión de los resultados por parte de los usuarios finales.

<<<<<<< HEAD
\section{Conclusiones y Perspectivas Futuras}

Este estudio demostró la factibilidad y relevancia del uso de datos colaborativos para la gestión inteligente del tráfico urbano en Antofagasta. A partir de más de 53.000 eventos registrados por usuarios de \textit{Waze}, se desarrolló un sistema integral capaz de analizar, predecir y visualizar el comportamiento del tránsito vehicular.

Los principales resultados obtenidos son:
\begin{itemize}
\item Se identificaron con precisión zonas críticas de alta concentración de accidentes, particularmente en los mencionados en \autoref{ssec:critical_zones} del eje Pedro Aguirre Cerda–Edmundo Pérez Zujovic.
\item Se confirmó una correlación estadísticamente significativa entre congestión y accidentes ($R^2 = 0.92$, $p < 0.001$ en días laborales), reforzada por el modelo exponencial inverso ($R^2 = 0.95$).
\item El modelo \textit{XGBoost} alcanzó un \textbf{F1-score de 0.88}, \textbf{Recall de 0.91} y \textbf{ROC–AUC de 0.84}, validando su capacidad predictiva.
\item El sistema fue implementado con un \textit{dashboard} operativo en línea, actualizado cada cinco minutos, que permite monitorear y proyectar eventos de tráfico en tiempo real.
\end{itemize}

Estos resultados confirman que la integración de datos colaborativos y técnicas de aprendizaje automático constituye una alternativa viable, escalable y de bajo costo para fortalecer la gestión vial en ciudades intermedias.

\subsection*{Recomendaciones y proyecciones futuras}

\begin{itemize}
\item \textbf{Intervención focalizada:} Priorizar la implementación de mejoras viales y medidas de seguridad en los segmentos críticos identificados.
\item \textbf{Validación empírica:} Incorporar registros municipales, cámaras de tránsito o estudios de terreno para corroborar los patrones detectados.
\item \textbf{Ampliación del modelo:} Integrar variables adicionales (meteorología, obras viales, visión por computadora) y explorar arquitecturas secuenciales como LSTM o modelos \textit{transformers}.
\item \textbf{Replicabilidad:} Adaptar la metodología a otras ciudades con dinámicas similares de congestión, ajustando los parámetros de calibración y fuentes de datos.
\item \textbf{Sostenibilidad del sistema:} Mantener el ciclo de reentrenamiento mensual del modelo y fortalecer la infraestructura de captura y procesamiento de datos.
\end{itemize}

En síntesis, el proyecto constituye una base sólida para el desarrollo de plataformas predictivas de movilidad urbana, demostrando que los datos colaborativos —combinados con técnicas de inteligencia artificial— pueden transformarse en herramientas efectivas para la toma de decisiones públicas.

\newpage

\printbibliography
=======
\section{Conclusiones}

Este estudio demuestra la factibilidad y el impacto del uso de datos colaborativos para la gestión inteligente del tráfico urbano en Antofagasta. Los principales aportes incluyen:

\begin{itemize}
    \item Identificación precisa de zonas críticas con alta concentración de accidentes.
    \item Confirmación de una correlación estadísticamente significativa entre congestión y accidentes, especialmente durante días laborales.
    \item Validación de modelos no lineales y de aprendizaje automático como herramientas para la predicción de incidentes en contextos urbanos densos.
    \item Desarrollo e implementación de un sistema funcional de monitoreo, visualización y actualización periódica del modelo, con potencial de escalabilidad.
\end{itemize}

El enfoque propuesto es viable y adaptable, constituyendo una base sólida para futuras iniciativas de movilidad urbana orientadas a la seguridad y eficiencia.

\section{Recomendaciones}

En base a los hallazgos, se sugiere:

\begin{itemize}
    \item \textbf{Intervención focalizada}: Priorizar mejoras de infraestructura, señalización y fiscalización en los segmentos críticos identificados.
    \item \textbf{Monitoreo y actualización}: Mantener y ampliar la infraestructura de captura y análisis de datos, integrando nuevas fuentes (sensores, cámaras) y promoviendo la actualización continua del modelo.
    \item \textbf{Validación cruzada}: Complementar los análisis con estudios de campo y encuestas a usuarios para validar y enriquecer los hallazgos.
    \item \textbf{Escalabilidad}: Replicar y adaptar la metodología a otras ciudades chilenas con características similares, atendiendo a sus particularidades.
    \item \textbf{Investigación avanzada}: Incorporar nuevas variables (clima, infraestructura, comportamiento humano) y explorar modelos predictivos de mayor complejidad y capacidad explicativa.
\end{itemize}

Estas recomendaciones buscan fortalecer una gestión vial proactiva y basada en datos, orientada a la reducción de accidentes y la mejora sostenida de la movilidad urbana.
>>>>>>> 52e04f0 (feat: update models comparation)

\newpage

\appendix
<<<<<<< HEAD

\pagenumbering{gobble}
\setcounter{page}{1}

=======
>>>>>>> 52e04f0 (feat: update models comparation)
\section{Transformación de coordenadas UTM}
\label{ap:utm}

La proyección UTM utilizada en este estudio (EPSG:32719) se basa en la proyección Transversa de Mercator, adaptada al hemisferio sur y centrada en el meridiano $\lambda_0 = -69^\circ$. Esta proyección es conforme y está diseñada para minimizar la distorsión métrica en una zona longitudinal específica (zona 19S), siendo especialmente adecuada para representar regiones como Antofagasta.

A continuación se presentan las fórmulas que permiten convertir coordenadas geográficas $(\phi, \lambda)$ —latitud y longitud en radianes— a coordenadas planas proyectadas $(x, y)$ en metros, basadas en el elipsoide WGS84:

\subsection*{Parámetros del elipsoide WGS84}

\begin{itemize}
  \item Semieje mayor: $a = 6378137.0$ m
  \item Aplanamiento: $f = \frac{1}{298.257223563}$
  \item Excentricidad: $e^2 = 2f - f^2$
  \item Factor de escala: $k_0 = 0.9996$
  \item Falso este: $E_0 = 500\,000$ m
  \item Falso norte (hemisferio sur): $N_0 = 10\,000\,000$ m
\end{itemize}

\subsection*{Variables intermedias}

\begin{align*}
e'^2 &= \frac{e^2}{1 - e^2} \\
N &= \frac{a}{\sqrt{1 - e^2 \sin^2 \phi}} \\
T &= \tan^2 \phi \\
C &= e'^2 \cos^2 \phi \\
A &= (\lambda - \lambda_0) \cos \phi
\end{align*}

\subsection*{Cálculo del arco meridiano}

\begin{align*}
M &= a \cdot \left[
(1 - \frac{e^2}{4} - \frac{3e^4}{64} - \frac{5e^6}{256}) \phi \right. \\
&\quad - (\frac{3e^2}{8} + \frac{3e^4}{32} + \frac{45e^6}{1024}) \sin(2\phi) \\
&\quad + (\frac{15e^4}{256} + \frac{45e^6}{1024}) \sin(4\phi) \\
&\quad \left. - (\frac{35e^6}{3072}) \sin(6\phi)
\right]
\end{align*}

\subsection*{Coordenadas proyectadas}

\begin{equation}
\begin{split}
y &= N_0 + k_0 \left[ M + N \tan \phi \left( \frac{A^2}{2} \right.\right. \\
&\quad \left.\left. + \frac{A^4}{24}(5 - T + 9C + 4C^2) + \frac{A^6}{720}(61 - 58T + T^2 + 600C - 330e'^2) \right) \right]
\end{split}
\end{equation}

<<<<<<< HEAD
Estas expresiones corresponden a un desarrollo en serie basado en una expansión de Taylor, que permite una transformación precisa desde la superficie curva del elipsoide a un plano cartesiano. Estas fórmulas son utilizadas internamente por la biblioteca PROJ, y fueron documentadas formalmente por Snyder \parencite{snyder1987}.
=======
Estas expresiones corresponden a un desarrollo en serie basado en una expansión de Taylor, que permite una transformación precisa desde la superficie curva del elipsoide a un plano cartesiano. Estas fórmulas son utilizadas internamente por la biblioteca PROJ, y fueron documentadas formalmente por Snyder \citep{snyder_tm}.
>>>>>>> 52e04f0 (feat: update models comparation)

\subsection*{Nota}

La implementación computacional de estas fórmulas puede realizarse utilizando bibliotecas como \texttt{Pyproj}, que encapsulan estos desarrollos y gestionan automáticamente los parámetros de cada sistema de referencia (CRS).

<<<<<<< HEAD
\section{Código fuente: Filtrado temporal y agrupación de eventos}
\label{annex:filter_code}

\begin{minted}[fontsize=\footnotesize,breaklines,frame=lines]{python}
def filter_by_group_time(self, timedelta_min: int, inplace: bool = False) -> gpd.GeoDataFrame | pd.DataFrame:
    """
    Filter and group data by time intervals.
    """
    if self.data is None or "pub_millis" not in self.data.columns:
        return pd.DataFrame()

    data = self.data.copy() if not inplace else self.data

    if not np.issubdtype(data["pub_millis"].dtype, np.integer):
        data["pub_millis"] = round(data["pub_millis"].astype(np.int64, errors="ignore") / 1_000_000).astype(np.int64)

    step = np.int64(60_000 * timedelta_min)
    data["interval_start"] = ((data["pub_millis"]).to_numpy() // step) * step

    data["interval_start"] = pd.to_datetime(data["interval_start"], unit="ms", utc=True)
    data["interval_start"] = data["interval_start"].dt.tz_convert(utils.TZ)

    data["group"] = data["group"].astype(np.int16)
    data["type"] = data["type"].astype(str)

    data.drop_duplicates(subset=["interval_start", "group", "type"], inplace=True)
    data.drop(columns=["interval_start"], inplace=True)

    if not pd.api.types.is_datetime64_any_dtype(data["pub_millis"]):
        data["pub_millis"] = pd.to_datetime(data["pub_millis"], unit="ms", utc=True)
        data["pub_millis"] = data["pub_millis"].dt.tz_convert(utils.TZ)

    data.reset_index(drop=True, inplace=True)
    return data
\end{minted}

Código: \\
\url{https://github.com/richardhapb/antof-traffic-client/blob/72c1e2cea730670bffb58f0844cd71d1888481df/src/analytics/alerts.py#L100}

\noindent\textbf{Nota:}
La función utiliza la zona horaria definida en el módulo \texttt{utils.TZ} y mantiene la consistencia tipológica de las columnas de entrada.

Referencias: \\
\url{https://github.com/richardhapb/antof-traffic-client/blob/72c1e2cea730670bffb58f0844cd71d1888481df/src/utils/utils.py#L27}

\newpage

\section{Código fuente: Generación de eventos negativos}
\label{annex:neg_code}

\begin{minted}[fontsize=\footnotesize,breaklines,frame=lines]{python}
def generate_neg_simulated_data(data: pd.DataFrame | GeoDataFrame, geodata: str = "group") -> GeoDataFrame:
    """
    Genera datos negativos simulando puntos sin evento.
    Basado en la adaptación del método de Goedertier et al. (2009).
    """
    # Conversión temporal y generación de intervalos de 5 minutos
    if not isinstance(data["pub_millis"].iloc[0], np.integer):
        data["pub_millis"] = data["pub_millis"].astype(np.int64, errors="ignore") / 1_000_000

    step = np.int64(60_000 * 5)
    min_tms = data["pub_millis"].to_numpy().min()
    max_tms = data["pub_millis"].to_numpy().max()
    intervals = np.arange(min_tms, max_tms + step, step)

    # Combinación de intervalos, grupos y tipos de evento
    data["interval"] = ((data["pub_millis"].to_numpy() - min_tms) // step) * step + min_tms
    allgroups = data[geodata].unique()
    alltypes = data["type"].unique()
    combinations = pd.MultiIndex.from_product(
        [intervals, allgroups, alltypes], names=["interval", geodata, "type"]
    ).to_frame(index=False)

    # Eventos originales
    event_combinations = data[["uuid", "interval", geodata, "type", "x", "y"]]
    event_combinations["happen"] = 1
    event_combinations["location"] = join_coords(event_combinations)  # función auxiliar

    # Unión y muestreo balanceado
    merged = pd.merge(combinations, event_combinations, on=["interval", geodata, "type"], how="left")
    merged_pos = merged[merged["happen"] == 1]
    merged_neg = merged[merged["happen"].isna()].sample(len(merged_pos), random_state=42)
    full_data = pd.concat([merged_pos, merged_neg])

    # Agregación y etiquetado
    full_data["happen"] = full_data["happen"].fillna(0).astype(int)
    alerts = utils.generate_aggregate_data(full_data)  # módulo auxiliar
    return alerts.data
\end{minted}

Código: \\
\url{https://github.com/richardhapb/antof-traffic-client/blob/72c1e2cea730670bffb58f0844cd71d1888481df/src/analytics/ml.py#L172}

\noindent\textbf{Nota:}
Las funciones \texttt{join\_coords()} y \texttt{generate\_aggregate\_data()} son funciones auxiliares definidas en el módulo \texttt{utils}, utilizadas para concatenar coordenadas geográficas y agregar estadísticas por grupo usadas en el modelo final, respectivamente.

Referencias: \\
\url{https://github.com/richardhapb/antof-traffic-client/blob/72c1e2cea730670bffb58f0844cd71d1888481df/src/utils/utils.py#L260} \\\\
\url{https://github.com/richardhapb/antof-traffic-client/blob/72c1e2cea730670bffb58f0844cd71d1888481df/src/utils/utils.py#L95}

\newpage

\section{Estructura general del sistema y despliegue}
\label{annex:dashboard}

El sistema desarrollado se compone de dos componentes principales: un cliente, implementado en \textit{Python}, y un servidor, desarrollado en \textit{Rust}.
Ambos módulos operan de manera conjunta para procesar los datos de tráfico, ejecutar los modelos de predicción y ofrecer un entorno de visualización accesible mediante un \textit{dashboard} web.

\subsection*{Cliente}

El cliente reúne los análisis exploratorios, la preparación de datos, el entrenamiento de los modelos y la interfaz visual.
Su estructura principal se divide en los siguientes módulos:

\begin{itemize}
    \item \textbf{Módulo \texttt{analytics}}: contiene las rutinas de análisis estadístico, correlaciones, agrupamiento y entrenamiento de los modelos predictivos.
    \item \textbf{Módulo \texttt{dashboard}}: implementa el panel de visualización construido con \textit{Dash}, que integra filtros por calle, fecha y tipo de evento, junto con parámetros ajustables para generar predicciones personalizadas.
    \item \textbf{Módulo \texttt{utils}}: agrupa funciones auxiliares para la limpieza y exploración de los datos.
    \item \textbf{Módulo \texttt{waze}}: maneja la ingesta de reportes provenientes de la API de Waze, los cuales sirven como fuente principal del conjunto de datos.
\end{itemize}

La ejecución del cliente se gestiona mediante el archivo principal \texttt{main.py}, que coordina el flujo completo: desde la carga de datos hasta la visualización final en el \textit{dashboard}.
El sistema fue configurado para su despliegue con \texttt{gunicorn} y puede ejecutarse tanto en entorno local como remoto.

\subsection*{Servidor}

El servidor fue desarrollado en \textit{Rust} e implementa una arquitectura ligera basada en un servicio web que actúa como intermediario entre el modelo predictivo y la interfaz visual.
Sus principales responsabilidades incluyen la administración de las solicitudes del \textit{Cliente}, el acceso a la base de datos y la gestión de caché.

\subsection*{Despliegue y acceso}

El sistema completo se encuentra desplegado en línea y disponible públicamente en \url{https://traficoantofagasta.com}, donde los usuarios pueden consultar tanto las métricas descriptivas como las predicciones generadas por el modelo.
El diseño modular del proyecto permite su actualización y ampliación futura, integrando nuevas fuentes de datos o modelos de predicción más avanzados.

\newpage

=======

\newpage


\bibliography{biblio}
>>>>>>> 52e04f0 (feat: update models comparation)
\end{document}

