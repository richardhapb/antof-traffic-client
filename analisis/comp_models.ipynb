{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2207fed3",
   "metadata": {},
   "source": [
    "\n",
    "Models to test:\n",
    "- RandomForestClassifier\n",
    "- LogisticRegression\n",
    "- XGBClassifier\n",
    "\n",
    "Encoded field (category)\n",
    "- type\n",
    "\n",
    "Encode method:\n",
    "- OneHotEncoder\n",
    "\n",
    "Target field: happen\n",
    "Values: 1 - true, 0 - false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b6ec80",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path().resolve()\n",
    "src_path = str(root.joinpath(Path(\"src\")))\n",
    "\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
<<<<<<< HEAD
    "from utils.utils import ALERTS_END_TIMESTAMP, ALERTS_BEGIN_TIMESTAMP, get_data, freq_nearby\n",
    "from analytics.ml import ML\n",
    "\n",
    "GRID_SEARCH = True"
=======
    "from utils.utils import ALERTS_END_TIMESTAMP, ALERTS_BEGIN_TIMESTAMP, get_data, generate_aggregate_data\n",
    "from analytics.ml import ML"
>>>>>>> 52e04f0 (feat: update models comparation)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alerts = get_data(ALERTS_BEGIN_TIMESTAMP, ALERTS_END_TIMESTAMP)\n",
<<<<<<< HEAD
    "alerts.filter_by_group_time(120, True)\n",
    "alerts.data.shape[0]"
=======
    "alerts.data[\"type\"].head()"
>>>>>>> 52e04f0 (feat: update models comparation)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_data = ML.generate_neg_simulated_data(alerts.data)\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ca265",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "enc_type = ohe.fit_transform(full_data[[\"type\"]])\n",
    "data = full_data.drop(columns=[\"type\"], axis=1).reset_index(drop=True)\n",
    "enc_df = pd.DataFrame(enc_type.toarray(), columns=ohe.get_feature_names_out()).reset_index(drop=True)\n",
    "data = pd.concat([data, enc_df], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a30e3c",
   "metadata": {},
   "source": [
    "Train/test split, hyperparameter search, evaluation, and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    "    confusion_matrix,\n",
    ")\n",
<<<<<<< HEAD
    "from sklearn.model_selection import StratifiedKFold\n",
=======
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
>>>>>>> 52e04f0 (feat: update models comparation)
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Build X, y\n",
    "assert \"happen\" in data.columns, \"Target column 'happen' not found.\"\n",
    "y = data[\"happen\"].astype(int).values\n",
    "X = data.drop(columns=[\"happen\"])\n",
    "\n",
    "# (Optional) columns safe for scaling (LogReg benefits; trees/XGB do not require it)\n",
    "# We'll scale only for LogisticRegression via a Pipeline.\n",
    "numeric_cols = X.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "X = X[numeric_cols]  # ensure purely numeric\n",
    "\n",
    "# --- Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fa422",
   "metadata": {},
   "source": [
    "Define models and grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b0b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
<<<<<<< HEAD
    "#\n",
    "# [RandomForest] best params: {'class_weight': None, 'max_depth': None, 'min_samples_leaf': 1, '\n",
    "# n_estimators': 500}\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1, max_depth=20, min_samples_leaf=3, n_estimators=500)\n",
=======
    "rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
>>>>>>> 52e04f0 (feat: update models comparation)
    "rf_grid = {\n",
    "    \"n_estimators\": [200, 350, 500],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 3],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "# LogisticRegression (with scaling in a Pipeline)\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),  # with_mean=False keeps sparse safety if present\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, random_state=RANDOM_STATE)),\n",
    "])\n",
    "lr_grid = {\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    \"clf__C\": [0.1, 1.0, 10.0],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "# [XGBoost] best params: {'colsample_bytree': 0.8, 'gamma': 0.8, 'learning_rate': 0.1, 'max_dept\n",
    "# h': 20, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}\n",
    "\n",
    "# XGBClassifier\n",
    "xgb = XGBClassifier(\n",
    "random_state=RANDOM_STATE,\n",
    "n_estimators=80,\n",
=======
    "# XGBClassifier\n",
    "xgb = XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_estimators=400,\n",
>>>>>>> 52e04f0 (feat: update models comparation)
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"auc\",\n",
    "    n_jobs=-1,\n",
<<<<<<< HEAD
    "    colsample_bytree=0.8,\n",
    "    gamma=1.0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=10,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
=======
>>>>>>> 52e04f0 (feat: update models comparation)
    ")\n",
    "xgb_grid = {\n",
    "    \"max_depth\": [3, 6, 10, 20],\n",
    "    \"n_estimators\": [80, 100],\n",
    "    \"learning_rate\": [0.1, 0.03],\n",
    "    \"gamma\": [0.8, 1.0],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "    \"min_child_weight\": [1, 3],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e489c9",
   "metadata": {},
   "source": [
    "Run GridSearchCV (ROC-AUC as primary metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def tune(model, grid, X, y, name):\n",
    "    gs = GridSearchCV(estimator=model, param_grid=grid, scoring=\"roc_auc\", cv=cv, n_jobs=-1, verbose=0)\n",
    "    gs.fit(X, y)\n",
    "    print(f\"[{name}] best CV ROC-AUC: {gs.best_score_:.4f}\")\n",
    "    print(f\"[{name}] best params: {gs.best_params_}\\n\")\n",
    "    return gs.best_estimator_\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "if GRID_SEARCH:\n",
    "    best_rf = tune(rf, rf_grid, X_train, y_train, \"RandomForest\")\n",
    "    best_lr = tune(lr_pipeline, lr_grid, X_train, y_train, \"LogisticRegression\")\n",
    "    best_xgb = tune(xgb, xgb_grid, X_train, y_train, \"XGBoost\")\n",
    "else:\n",
    "    best_rf = rf.fit(X_train, y_train)\n",
    "    best_xgb = xgb.fit(X_train, y_train)"
=======
    "best_rf = tune(rf, rf_grid, X_train, y_train, \"RandomForest\")\n",
    "best_lr = tune(lr_pipeline, lr_grid, X_train, y_train, \"LogisticRegression\")\n",
    "best_xgb = tune(xgb, xgb_grid, X_train, y_train, \"XGBoost\")"
>>>>>>> 52e04f0 (feat: update models comparation)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4688b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Evaluate on the test set and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def evaluate(model, X_te, y_te, name):\n",
    "    proba = model.predict_proba(X_te)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    return OrderedDict(\n",
    "        model=name,\n",
    "        accuracy=accuracy_score(y_te, pred),\n",
    "        precision=precision_score(y_te, pred, zero_division=0),\n",
    "        recall=recall_score(y_te, pred, zero_division=0),\n",
    "        f1=f1_score(y_te, pred, zero_division=0),\n",
    "        roc_auc=roc_auc_score(y_te, proba),\n",
    "        pr_auc=average_precision_score(y_te, proba),\n",
    "    )\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "if GRID_SEARCH:\n",
    "    models = [(best_rf, \"RandomForest\"), (best_lr, \"LogisticRegression\"), (best_xgb, \"XGBoost\")]\n",
    "else:\n",
    "    models = [(best_rf, \"RandomForest\"), (best_xgb, \"XGBBoost\")]\n",
    "\n",
=======
    "models = [(best_rf, \"RandomForest\"), (best_lr, \"LogisticRegression\"), (best_xgb, \"XGBoost\")]\n",
>>>>>>> 52e04f0 (feat: update models comparation)
    "results = []\n",
    "for m, n in models:\n",
    "    results.append(evaluate(m, X_test, y_test, n))\n",
    "\n",
    "summary_df = pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False).reset_index(drop=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5150c26",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Confusion matrices (threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cbe47f",
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "lines_to_next_cell": 0
   },
>>>>>>> 52e04f0 (feat: update models comparation)
   "outputs": [],
   "source": [
    "for model, name in models:\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"{name} — Confusion Matrix\")\n",
    "    print(cm)\n",
<<<<<<< HEAD
    "    print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0612fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR curves\n",
    "size = (12, 4.7)\n",
    "fig = plt.figure(figsize=size)\n",
    "axes = fig.subplots(1, len(models))\n",
    "for i, (model, name) in enumerate(models):\n",
    "    RocCurveDisplay.from_estimator(model, X_test, y_test, name=name, ax=axes[i], color=\"red\")\n",
    "    axes[i].set_title(f\"ROC Curve\\n{name}\")\n",
    "    axes[i].legend(fontsize=10, loc=\"lower center\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graph/ROC.png\")\n",
    "\n",
    "fig = plt.figure(figsize=size)\n",
    "axes = fig.subplots(1, len(models))\n",
    "for i, (model, name) in enumerate(models):\n",
    "    PrecisionRecallDisplay.from_estimator(model, X_test, y_test, name=name, ax=axes[i], color=\"orange\")\n",
    "    axes[i].set_title(f\"Precision-Recall Curve\\n{name}\")\n",
    "    axes[i].legend(fontsize=10, loc=\"lower center\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graph/Precision-Recall.png\")"
=======
    "    print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b474a",
   "metadata": {},
   "source": [
    "ROC and PR curves\n",
    "\n",
    "plt.figure()\n",
    "for model, name in [(best_rf, \"RandomForest\"), (best_lr, \"LogisticRegression\"), (best_xgb, \"XGBoost\")]:\n",
    "    RocCurveDisplay.from_estimator(model, X_test, y_test, name=name)\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for model, name in [(best_rf, \"RandomForest\"), (best_lr, \"LogisticRegression\"), (best_xgb, \"XGBoost\")]:\n",
    "    PrecisionRecallDisplay.from_estimator(model, X_test, y_test, name=name)\n",
    "plt.title(\"Precision-Recall Curves\")\n",
    "plt.show()"
>>>>>>> 52e04f0 (feat: update models comparation)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf9b4e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    " Feature importance for tree models (top 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5a987",
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "lines_to_next_cell": 2
   },
>>>>>>> 52e04f0 (feat: update models comparation)
   "outputs": [],
   "source": [
    "def show_feature_importance(model, feature_names, title):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        order = np.argsort(importances)[::-1][:15]\n",
    "        top_feats = np.array(feature_names)[order]\n",
    "        top_vals = importances[order]\n",
    "        plt.figure()\n",
    "        plt.barh(range(len(top_feats)), top_vals[::-1])\n",
    "        plt.yticks(range(len(top_feats)), top_feats[::-1])\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
<<<<<<< HEAD
    "show_feature_importance(best_rf, X_train.columns, \"RandomForest Feature Importance (Top 15)\")\n",
    "show_feature_importance(best_xgb, X_train.columns, \"XGBoost Feature Importance (Top 15)\")"
=======
    "\n",
    "# RandomForest\n",
    "show_feature_importance(best_rf, X_train.columns, \"RandomForest Feature Importance (Top 15)\")\n",
    "\n",
    "# XGBoost (note: best_xgb is the estimator directly)\n",
    "show_feature_importance(best_xgb, X_train.columns, \"XGBoost Feature Importance (Top 15)\")\n",
    "\n"
>>>>>>> 52e04f0 (feat: update models comparation)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
